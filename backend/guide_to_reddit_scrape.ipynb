{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/scraping-reddit-data-1c0af3040768\n",
    "\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='OzDq72tPFF_sBQ', client_secret='cZEK5LrHFW_GBqylCsdOT9H6bM5xDQ', user_agent='Reddit WebScrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D] Simple Questions Thread December 20, 2020\n",
      "[D] Machine Learning - WAYR (What Are You Reading) - Week 104\n",
      "[D] I'm trying to do more stuff in pure Tensorflow. Is there an in-depth book that explain constructing recurrent, convolutional, graph etc layers in it?\n",
      "[Discussion] How much responsibility do people who work on ML have to combat & disclose potential negative effects of AI online? (e.g. potential extremism, addiction, violence amplification)\n",
      "[D] Machine Learning category in Google Scholar Metrics\n",
      "[P] An old project of mine created back in 2005. It's a robotic arm moved by a neural network. Trained using genetic algorithms. Targets/scores are assigned using a scripting language.\n",
      "[Discussion] good html tokenization libraries?\n",
      "[R] SQL injection as a reinforcement learning challenge\n",
      "[D] App to create a dictionary with scientific terms and references\n",
      "[P] Which model should I use to help choosing the best answer for the TOEIC reading test\n"
     ]
    }
   ],
   "source": [
    "hot_posts = reddit.subreddit('MachineLearning').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Endgame.\n",
      "Fever [OC]\n",
      "Agreed\n",
      "Reddit, meet Bowie!\n",
      "Shhh let him sleep\n",
      "Jim Cramer admitting to how he manipulated the short selling market back in 2006. This needs to be seen by all!\n",
      "You just gotta put enough flakes in the leftover milk 😭\n",
      "collection\n",
      "I'm 17 and was often called fat by my friends. To the left is me 9 months ago weighing 97Kg. In the right I'm down to 68Kg. I know my muscles aren't as big but I'm glad with what I have achieved so far :)\n",
      "Biden, Democrats hit gas on push for $15 minimum wage\n"
     ]
    }
   ],
   "source": [
    "hot_posts = reddit.subreddit('all').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score      id  \\\n",
      "0      [D] Simple Questions Thread December 20, 2020     32  kh2b81   \n",
      "1  [D] Machine Learning - WAYR (What Are You Read...     28  kzevku   \n",
      "2  [D] I'm trying to do more stuff in pure Tensor...     55  l8htiy   \n",
      "3  [Discussion] How much responsibility do people...      6  l8n1ic   \n",
      "4  [D] Machine Learning category in Google Schola...      9  l8knn8   \n",
      "5  [P] An old project of mine created back in 200...      4  l8p9x1   \n",
      "6     [Discussion] good html tokenization libraries?      5  l8lwry   \n",
      "7  [R] SQL injection as a reinforcement learning ...    251  l7rrhs   \n",
      "8  [D] App to create a dictionary with scientific...      1  l8q5v0   \n",
      "9  [P] Which model should I use to help choosing ...      1  l8pjgr   \n",
      "\n",
      "         subreddit                                                url  \\\n",
      "0  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "1  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "2  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "3  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "4  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "5  MachineLearning                    https://v.redd.it/2nbh9z3zxhe61   \n",
      "6  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "7  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "8  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "9  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "\n",
      "   num_comments                                               body  \\\n",
      "0           408  Please post your questions here instead of cre...   \n",
      "1             5  This is a place to share machine learning rese...   \n",
      "2            16  I'm making this post here because I just read ...   \n",
      "3             5  It was difficult for me to decide to [write](h...   \n",
      "4             5  Google Scholar Metrics does not have machine l...   \n",
      "5             0                                                      \n",
      "6             1  I'm working on an information retrieval projec...   \n",
      "7            12  New work by researchers at University of Oslo ...   \n",
      "8             0  Hi,\\n\\nI started creating a dictionary in note...   \n",
      "9             0  I want to build a question answering model tha...   \n",
      "\n",
      "        created  \n",
      "0  1.608527e+09  \n",
      "1  1.610946e+09  \n",
      "2  1.612026e+09  \n",
      "3  1.612046e+09  \n",
      "4  1.612038e+09  \n",
      "5  1.612053e+09  \n",
      "6  1.612043e+09  \n",
      "7  1.611953e+09  \n",
      "8  1.612055e+09  \n",
      "9  1.612054e+09  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "for post in ml_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/MachineLearning/comments/kh2b81/d_simple_questions_thread_december_20_2020/\n",
      "https://www.reddit.com/r/MachineLearning/comments/kzevku/d_machine_learning_wayr_what_are_you_reading_week/\n",
      "https://www.reddit.com/r/MachineLearning/comments/l8htiy/d_im_trying_to_do_more_stuff_in_pure_tensorflow/\n",
      "https://v.redd.it/2nbh9z3zxhe61\n",
      "https://www.reddit.com/r/MachineLearning/comments/l8n1ic/discussion_how_much_responsibility_do_people_who/\n",
      "https://www.reddit.com/r/MachineLearning/comments/l8knn8/d_machine_learning_category_in_google_scholar/\n",
      "https://www.reddit.com/r/MachineLearning/comments/l8lwry/discussion_good_html_tokenization_libraries/\n",
      "https://www.reddit.com/r/MachineLearning/comments/l8pjgr/p_which_model_should_i_use_to_help_choosing_the/\n",
      "https://www.reddit.com/gallery/l8nn3u\n",
      "https://www.reddit.com/r/MachineLearning/comments/l7rrhs/r_sql_injection_as_a_reinforcement_learning/\n"
     ]
    }
   ],
   "source": [
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "for post in ml_subreddit.hot(limit=10):\n",
    "    print(post.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission(id = \"8olsjgun89e61\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "received 404 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-95993078395f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_more\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Toplevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtop_level_comment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_level_comment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/praw/models/reddit/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"\"\"Return the value of `attribute`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         raise AttributeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0msubmission_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomment_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files, json)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At most one of `data` and `json` is supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             return self._core.request(\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"api_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         return self._request_with_retries(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    263\u001b[0m             )\n\u001b[1;32m    264\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTATUS_EXCEPTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no_content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFound\u001b[0m: received 404 HTTP response"
     ]
    }
   ],
   "source": [
    "submission.comments.replace_more(limit=0) #Toplevel\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What tooling do you use for custom image tagging and labelling? \n",
      " \n",
      "I have a bunch of unique imagines and want something thats lean, well documented, and plays nice with current languages/libraries/data formats. \n",
      "\n",
      "What are your goto's, tried and trusted?\n",
      "In section 14.1 \\[0\\] of the Deep Learning book it says that \"When the decoder is linear and L is the mean squared error, an undercomplete autoencoder learns to span the same subspace as PCA\". I could not find a source/proof for this statement. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "This is clearly exactly PCA with a one layer encoder and one layer decoder. But the connection with multi-layer autoencoders is not clear to me. If someone could explain or point me in the right direction where to read about the above statement that would be great!\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "\\[0\\] [https://www.deeplearningbook.org/contents/autoencoders.html](https://www.deeplearningbook.org/contents/autoencoders.html)\n",
      "Is the Book \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio and Aaron Courville a good resource in 2020 or are there other, better ones? I'd like to hear some opinions and suggestions otherwise.\n",
      "Is [the mathematics for machine learning specialization offered by Imperial College London](https://www.coursera.org/specializations/mathematics-machine-learning) sufficient for maths for machine learning from someone from non-cs background. Specially, I am taking the Machine Learning course by Andrew Ng.\n",
      "In the context of artificial general intelligence, what do you think is the most viable alternative to today's commonly used artificial neural network architecture if computational resources and speed isn't (too much of) an issue? (e.g. Numenta's SDR, Deep Bayesian, hybrid architecture of sort, etc.)\n",
      "How do I clone a git repository and start using predefined models to generate sounds/music/noises, etc...\n",
      "\n",
      "I would like to train a model using existing ASMR noises to generate new sounds. Something like this video here: [https://youtu.be/w926Afa1HbY](https://youtu.be/w926Afa1HbY)\n",
      "\n",
      "Also, any recommendations on what to use? Magenta, MelGan, or something else?\n",
      "I am not sure if my question fits more in /r/cscareerquestions/ \n",
      "\n",
      "I will start a bachelor degree in september next year here in switzerland (BSC in AI & ML). I am 25M with about 4 years job experience in IT, what are some useful prep courses online or with books that I can start now.\n",
      "\n",
      "I want to finish my degree with honors while continuing to work 60% at my current workplace. Currently Im studying linear algebra with https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/syllabus/ (is this outdated?) and for python I would start with Jose Portilla's Udemy courses as soon as they get discounted again.\n",
      "\n",
      "I still need something to learn statistics 6 with, the problem I have is that I dont know what the 6 means. Does it mean 6th grade or 6th part of something?\n",
      "\n",
      "Any other recommendations of things I can do to prepare? This is the schools bachelor  [overview page](https://www.hslu.ch/en/lucerne-school-of-information-technology/degree-programs/bachelor/artificial-intelligence-and-machine-learning/) and the required  [modules](https://www.hslu.ch/en/lucerne-school-of-information-technology/degree-programs/bachelor/artificial-intelligence-and-machine-learning/module/).\n",
      "\n",
      "Thank you for your time.\n",
      "\n",
      "I will most likely follow this  [reddit post](https://www.reddit.com/r/learnmachinelearning/comments/jv4yrk/a_roadmap_to_andrew_ngs_cs229/)\n",
      "About Categorical Crossentropy loss: Wouldn't simply predicting a 1 for all classes always lead to zero loss, since log(1)==0?\n",
      "\n",
      "Is it just the assumption that a soft max layer before that avoids cheating in that way, and that makes Categorical CE work?\n",
      "I have never used ML but am supposed to cut my teeth on a project at work using ML soon. I will be given a set of photos (about 15k) taken from security like cameras at about 10 locations taking photos of a wetland area (marsh/lagoon). The cameras take photos over consistent intervals of time. My task is to build a ML model that can look at the photos and count how many birds are in each photo. It does not need to identify species of birds, just return a count of birds in each photo. \n",
      "\n",
      "My question is what type of machine learning will I be using? I have looked online and it seems like I will need to use Image Localization. While I don't necessarily need to have highly accurate boxes around the birds, I am assuming once it bounds all the birds in a photo into boxes it is very easy to retrieve counts. Is this accurate or is there another type of model I will use? I should also note I am proficient with python and will be using python primarily for this project.\n",
      "\n",
      "In addition to any feedback on my question, I would really appreciate it if anyone had any good learning resources. I am considering buying [this book](https://machinelearningmastery.com/deep-learning-for-computer-vision/), but if anyone has any other/better recommendations I would really appreciate it! \n",
      "\n",
      "Thank you in advance for your time!\n",
      "I'm v. new to Tensorflow. Is there any way of predicting what operations are fast, other than implementing them every way imaginable and profiling?\n",
      "\n",
      "Here's the concrete case I'm dealing with. I have a Nx9x9 Tensorflow tensor (representing information about a series of Sudoku boards). I want to average elements along consecutive triples of elements, like so:\n",
      "\n",
      "aaabbbccc\n",
      "\n",
      "dddeeefff\n",
      "\n",
      "...\n",
      "\n",
      "I can think of 3 ways of reducing from 9x9 to 3x9:\n",
      "\n",
      "\\* convolution with a 3x1 kernel with weights (0.33, 0.33, 0.33) and stride 3\n",
      "\n",
      "\\* reshape to 9x3x3 + reduce\\_mean on axis 2\n",
      "\n",
      "\\*image rescaling ops (maybe?)\n",
      "\n",
      "But as noted, I have no intuition for which is the 'right' approach here, and (more importantly) I want to be able to move from trial and error to reasoning about performance. I'd be grateful for any advice.\n",
      "Please advise the most appropriate community or thread where I can post our request for volunteer developers ? (to help with open source project)\n",
      "\n",
      "Sky Hub AI UAP Tracker - Development tasks\n",
      "\n",
      "We need people with experience in go lang and vue.js. We are primarily using go for our backend, vue.js frontend. If you have experience in elasticsearch, mysql, vuejs, C you will also be able to get involved.\n",
      "[Sky Hub Chat Server](https://chat.skyhub.org)\n",
      "\n",
      "Many thanks \n",
      "Paul\n",
      "I seriously need help.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "So right now ( about a few weeks ago ), I started diving into ML with really no previous experience other than some average middle school and high school math/stats, and so far that has proven more than enough. This is mainly due to me being very good with Python, so I understand a lot of the concepts. ( also quick note, I am learning Keras for now as this seemed the most simple and easy way to get into ML ) However, every day I am learning more and more that there are so many more heavily advanced concepts that require so much stuff to learn. What should I expect to learn? And what would the timeline be like? Programatically I can handle most things with some moderate review, but really, I'm not so sure I'm gonna have such an amazing time grasping the concepts and math/stats of this all. Are there any resources out there that you guys would recommend to a beginner? \n",
      "\n",
      "Thank you so much for taking the time to read this. I hope you have an amazing day. :)\n",
      "Is there any research into modifying the pose of a subject in a photo?\n",
      "\n",
      "Input would be an image containing a person, and the output would be the same image except the person has (e.g.) a 'T' pose with their arms outstretched.\n",
      "If you run K-Fold cross validation, you get K models. Would it be sensible to average hyperparameters across all models and re-train one model across all data?\n",
      "I can't immediately go into machine learning from my bachelor degree. I have goals to eventually get a masters in ML, but I want to get real world work experience first for a few years. Should I get a job as a data analyst, data engineer or software engineer. Which one would be the most suitable pathway to eventually lead to become an ML engineer?\n",
      "Should I do Practical Deep Learning for Coders by Fast.ai or CS231n first? \n",
      "\n",
      "I'll probably do both but what order should I start to maximize my learning capacity to this courses.\n",
      "\n",
      "I'm done with Machine Learning by Andrew Ng (with the assistance of StatQuest and 3Blue1Brown, they really helped me sharpen my intuition as a newbie when it comes to Machine Learning.)\n",
      "\n",
      "Also someone suggested me to do Code-First Introduction to Natural Language Processing by Fast.ai and CS224n too, so I'll probably do that in the future. I hope I'm in the right track.\n",
      "I've just started learning how to make linear regression models and have a doubt related to it.\n",
      "\n",
      "The tutorial I found tells to use all the variables for the first attempt at making the model and then check the significance of the variables to filter out the ones which aren't significant.\n",
      "\n",
      "What I want to know is while including all variables, assuming we're analysing sales of different branches of a company, whether the branch number should be considered or not? And if it should be considered, should it be taken as a factor or a numerical value?\n",
      "Is there a place to discuss serious collaborations on computer vision research? Like mentorship from more senior researches for graduate or undergraduate students that would like to partner or expand their circle of collaborators?  \n",
      "\n",
      "\n",
      "I'm not even sure if that even makes sense, but as a MSc student with almost no one to discuss (my advisor actually specializes in another sub-field), I would love to collaborate with people from other countries, even if for simple discussion on machine and deep learning, and computer vision research and applications, to actual research-track projects.\n",
      "**TL;DR** Why are language modeling pre-training objectives considered unsupervised when we technically have ground-truth answers?\n",
      "\n",
      "Maybe this is stemming from my not-so-great grasp of supervised vs. unsupervised learning, but my understanding is that if we have access to ground-truth labels then it's supervised learning and if not then it's unsupervised.\n",
      "\n",
      "I'll take the masked language modeling (MLM) that BERT ([Devlin et al., 2019](https://www.aclweb.org/anthology/N19-1423/)) and many other subsequent language models use.\n",
      "\n",
      "According to the original paper:\n",
      "\n",
      "> ...we simply mask some percentage of the input tokens at random, and then predict those masked tokens... In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.\n",
      "\n",
      "If we just replace a certain percentage of tokens with `[MASK]` randomly, don't we technically have access to the ground-truth labels (i.e., the original unmasked tokens)? Shouldn't this be considered supervised learning?\n",
      "\n",
      "My argument is analogous for the next sentence prediction (NSP) task.\n",
      "What happens if you train a model with a particular batch shape (x,y,z,c), But when you build the model again, you use a different input batch shape? Does this affect the predictions of the model?\n",
      "If I hope to be involved in the field of robotics eventually, would it be best to start learning reinforcement learning? Is this the most relevant method of ML used for robotics? I’m a computing science student at a school that is known for reinforcement learning so I would like to complete a reinforcement learning project this summer and hopefully get involved with one of the labs at my university. Would this be a good path considering my interests in robotics?\n",
      "Hey all\n",
      "\n",
      "Hopefully this is a simple question. I'll try my best to explain what the reply I want will look like.\n",
      "\n",
      "I am building a segmented image dataset, I am working with a small team to construct the dataset.\n",
      "\n",
      "There were several examples of us miss labeling objects when compared to each others labels (e.g. there is a class for foot ball, one of our team members labels American football, the other labels European footballs. The class was originally intended only for European footballs)\n",
      "\n",
      "The way we are dealing with this issue is browsing the images one by one and visually inspecting if the labels are consistent.\n",
      "\n",
      "My question is, do you know of papers looking in to detecting outliers in segmented image data?\n",
      "\n",
      "I am expecting something similar to clustering or dimensionality reduction except applied to segmented image classes.  Let me know what you think\n",
      "\n",
      "Thank you very much for any help you can give\n",
      "Hello all, in deep learning, instead of early stopping with patience check, what if we decay the learning rate aggressively to try converge more to the minimum when we hit the patience check? Could it be a viable solution?\n",
      "Would anyone be willing to take on an apprentice or mentee? I would appreciate any help/resources/ideas as I’m a junior developer who is eager to learn and succeed. I am willing to put in whatever time to get better at machine learning\n",
      "Hello everyone, I am looking for a **web or desktop application** to create a dataset for **image segmentation**. I tried using Label Box, but nothing worked for me (the content of the exported file format is too complicated to understand, no examples). Also what is the standard format for such a dataset, how should images and masks be stored? Thanks in advance!\n",
      "When would we use a transformer encoder only (similar to BERT?), a transformer decoder only (similar to GPT?), or a transformer encoder-decoder (as proposed by Vaswani et al. in 2017)?  \n",
      "\n",
      "\n",
      "Excuse me if this is a shitty question that shows my lack of understanding of the literature behind transformers and self-attention based models but it's something that I've been wondering since Google posted their Vision Transformer. They only used the encoder part for their classification model. FB however used an encoder-decoder for their DETR.\n",
      "\n",
      "Similarly, from what I understand BERT only uses the encoder, GPT only uses the decoder section, while the original 'Attention is all you need' proposes the transformer as the model with both the encoder-decoder components. Are there any particular advantages or disadvantages, and situations where we should choose one specific component?\n",
      "Where do you read the news of ML research? As the field is rapidly growing, I find it difficult to keep up with the research. I'm mainly interested in NLP although I'm also seeking for a AI/ML/DL news source.\n",
      "\n",
      "I'd also like to know any other websites/aggregator you visit daily or weekly both purely scientific (paper reviews) and popular science.\n",
      "Hi there, I want to find all the brands that are sponsoring a video on YouTube (or a podcast). Right now we are using fuzzy queries on description and transcripts. But that does not cover all cases. \n",
      "\n",
      "Often content creators use terms like “this video is sponsored/brought to you by ....” but not always. So we want to use nlp. \n",
      "\n",
      "Please suggest any idea/library or any direction to further investigation. Any help is appreciated.\n",
      "Hi \n",
      "I want to start with artificial intelligence. I have a little background on programming and good background on math and statistics. Where should I start and what are the best sources to start with?\n",
      "On StyleGAN face images are aligned, I assume this gives the network result some kind of boost? How should I proceed if I have a dataset with non human faces, such as trees? How should they be aligned?\n",
      "I’m super new to ML, I have some ok python experience. I want to create or work on a model to predict stock movements. I’m not looking for a get rich quickly thing and I understand that it won’t get it right some of the time, I’m also ok with losing the investment. I’m basically looking for a tool that can identify some swing trades entry and exit points (I have no problem if the swing duration is days to months, I’m not looking to get rich, I am looking for the experience)\n",
      "\n",
      "My question: is this something that ML can accomplish with acceptable results? Or is this just a waste of time?\n",
      "i need a little helo with neural network with handwritten numbers\n",
      "\n",
      "Hello, i just start learning deep learning, and machine learning, but its a little hard to me, for understand python, and this, and i have a test to make an neural network with  handwritten numbers.\n",
      "\n",
      "This is the code i have for this.\n",
      "\n",
      "\\######################################################################################\n",
      "\n",
      "import tensorflow as tf  \n",
      "from tensorflow.keras.utils import to\\_categorical  \n",
      "(x\\_train, y\\_train), \\_ = tf.keras.datasets.mnist.load\\_data()  \n",
      "\n",
      "\n",
      "import numpy as np  \n",
      "import matplotlib.pyplot as plt  \n",
      "fig = plt.figure(figsize=(25, 4))  \n",
      "for idx in np.arange(20):  \n",
      "   ax = fig.add\\_subplot(2, 20/2, idx+1, xticks=\\[\\], yticks=\\[\\])  \n",
      "   ax.imshow(x\\_train\\[idx\\], cmap=plt.cm.binary)  \n",
      "   ax.set\\_title(str(y\\_train\\[idx\\]))  \n",
      "\n",
      "\n",
      "x\\_train = x\\_train.reshape(60000, 784).astype('float32')/255  \n",
      "y\\_train = to\\_categorical(y\\_train, num\\_classes=10)  \n",
      "\n",
      "\n",
      "model = tf.keras.Sequential()  \n",
      "model.add(tf.keras.layers.Dense(10,activation='sigmoid', input\\_shape=(784,)))  \n",
      "model.add(tf.keras.layers.Dense(10,activation='softmax'))  \n",
      "model.compile(loss=\"categorical\\_crossentropy\", optimizer=\"sgd\", metrics = \\['accuracy'\\])  \n",
      "model.fit(x\\_train, y\\_train, epochs=10, verbose=0)  \n",
      "\\_, (x\\_test\\_, y\\_test\\_)= tf.keras.datasets.mnist.load\\_data()  \n",
      "x\\_test = x\\_test\\_.reshape(10000, 784).astype('float32')/255  \n",
      "y\\_test = to\\_categorical(y\\_test\\_, num\\_classes=10)  \n",
      "test\\_loss, test\\_acc = model.evaluate(x\\_test, y\\_test)  \n",
      "print('Test accuracy:', test\\_acc)  \n",
      "image = 7  \n",
      "\\_ = plt.imshow(x\\_test\\_\\[image\\], cmap=plt.cm.binary)  \n",
      "import numpy as np  \n",
      "prediction = model.predict(x\\_test)  \n",
      "print(\"Model prediction: \", np.argmax(prediction\\[image\\]))\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "the only issue i have its i dont know how to add a neural network for this code, cand someone could help me with that?\n",
      "Anyone used NVidia Tesla K20x? I can get it super cheap, has nice FP64 performance, and I just want to know if it will be any better than Quadro 4000, GeForce GT 710?\n",
      "Is there a way to craete a confusion matrix but instead of vales (number of correctly/ not correctly predicted images) I want to put the actual images? like this matrix:  https://imgur.com/gallery/jOhWCzp\n",
      "I have a simple CNN model built with the help of keras library (not the one from tensorflow).I am able to train the model and check its accuracy against an already existing dataset. The weighted network has been created and stored in .h5 file. Now I would like to create an application (preferably using JS) which takes an input image and I give the output as the predicted result according to my model. Could you suggest any ways on how to do this?\n",
      "Hey i'm new to ML and have few questions.\n",
      "\n",
      "1. Is ML bad for my laptop, it's using Ryzen 7 4700U and Radeon Graphics, my laptop has a really thin bodies and doesn't stand against heat for a long time. Do i had to run my ML model locally, or should i not ?\n",
      "2. For cloud alternatives, i have used Google Colab and Kaggle kernel, are they good enough for you guys to do your thing, or is it too slow for real ML engineer ? What cloud services do you use for training ML ? (Especially the free one, for learning and competitions)\n",
      "Greetings fellow ML enthusiasts, I used to run my deep learning framework on a ubuntu PC, but now I want to \"spread the workload\" by running my model on my Windows 10 Laptop with GTX 1050ti 4 GB. My question is, can anyone please suggest a comprehensive and updated guide to set up a Deep Learning environment on Windows 10? I know there are plenty of guides out there, but I want to know if there is a specific guide that is \"well-known\" and \"endorsed\" by the ML community. Thank you in advance. Cheers.\n",
      "\n",
      "http://faculty.marshall.usc.edu/gareth-james/\n",
      "\n",
      "https://www.coursera.org/specializations/statistics\n",
      "\n",
      "I did my initial research for finding a statistical lecture with mix of R and so far I heard good things about this 2. Just want to know if whom should I prioritized first. Or is there any other else better than these 2?\n",
      "\n",
      "Thank you!\n",
      "Is there any \"ML\" way to predict a binary list? For example, given 100 binary inputs, predict the next 10. I could easily do it from a probabilistic perspective using Bernoulli, but I haven't been able to find a proper MachineLearning way to do so.\n",
      " \n",
      "\n",
      "Hi everyone. For a project I'm working on, I'm trying to train a model. The model takes 12 inputs and should output based on the number of classes(I'm starting with two classes). The 12 inputs represent different aspects that determine the position and gesture of my hand, and the two classes are two gestures(thumbs up and high five).\n",
      "\n",
      "Right now, my model just looks like this\n",
      "\n",
      "`12(Input) -> 7(Dense) -> 3(Dense) -> 2(Dense)`\n",
      "\n",
      "This model seems like it would work(although I'm really just a beginner at machine learning so correct me if it doesn't make sense) but the main problem is the lack of data. After spending some time gathering data, I ended up with 50 data samples for each class or 100 data samples in total. I know this is not near enough to train effectively. Right now, I can just get more data, but in the future, I want to be able to create a model on-the-fly using only 100 data points.\n",
      "\n",
      "How can I achieve this?\n",
      "\n",
      "tl;dr: I need to train the model above with a minimal amount of data, what are ways to do so?\n",
      "Hi everyone. I was working on a project related to Image Super resolution that converts an input LR image to HR using GANs. I was wondering whether it was a good idea to input an HR image to the model and take the loss between its output and the input. Will such a loss function be beneficial to the model in anyway?\n",
      "I don't know if this is suited to this subreddit - as its not true ML, so if you know of somewhere else for me to look, let me know.\n",
      "\n",
      "I have a multi-dimensional optimization task where I have access to a high performance computing ring to run individual tasks in parallel as well as to run them on my local machine. I have implemented/used a few optimization algorithms like MCS and Nelder-Mead that run iterations in series trying to minimize or maximize a cost function.\n",
      "\n",
      "I'm trying to find any reading or advice on what kind of methods can leverage parallel computation to reduce runtime for a task like this, especially for tasks where single iterations can take ~5minutes or longer. If anyone know where to ask this question if its better suited to another subreddit, let me know as well.\n",
      "\n",
      "It is important to note that this is Derivative Free optimization since I have no information about the cost function, other than its values where I evaluate them.\n",
      "Does anyone know a good library for few-shot learning? I'm not too good at ML and don't want to focus too much on it as it isn't the main part of my project. I looked at Reptile(OpenAI) but all of it went over my head and I couldn't tailor the example code to fit my case.\n",
      "Hi im currently taking DataCamp course on ML,but I don't feel that it will be enough.Can anyone recommend a book to learn machine learning while undertaking those courses?\n",
      "Is Rust worth learning to do ML?\n",
      "\n",
      "I'm a complete beginner to ML, but I'm familiar with Python and have already some real experience with programming (mainly in mobile development). I want to start learning ML, and I thought that it would be a nice pretext for learning Rust too, as it seems to have already some kind of environment to accomplish the basic tasks.\n",
      "Can someone explain backpropagation in context of neural network. I've been trying to find a good explanation, but for some reason, the explanation is not clicking with me. Could someone offer a \"good, clear, concise explanation\" of backpropagation \"for dummies\" edition\n",
      "Help finding a particular paper.\n",
      "\n",
      "The network is fed images rendered from a player's perspective within a maze, such that the camera rotates 360deg for a full view of the surroundings.\n",
      "\n",
      "As this happens, we see a heatmap superimposed on a top-down image of the maze. The \"hot\" regions eventually converge into a single point which is the network's guess for the player's position.\n",
      "\n",
      "Iirc, the authors ran it on rendered mazes and real-life environments\n",
      "Just started dealing with medical images, and the images are in series.\n",
      "\n",
      "I searched some 3D networks and found lots of them designed for time-sequence data not image series.\n",
      "\n",
      "So i wonder can I just replace all operations from 2D networks to 3D (convs, pooling,etc), or is there something else different in '3D' from 2D single images?\n",
      "I am using the following code to try to run [StyleGAN](https://github.com/NVlabs/stylegan) on Google Colab: [https://github.com/jeffheaton/t81\\_558\\_deep\\_learning/blob/master/t81\\_558\\_class\\_07\\_3\\_style\\_gan.ipynb](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_style_gan.ipynb)\n",
      "\n",
      "and running into the error \"AttributeError: module 'dnnlib' has no attribute 'SubmitConfig\" after running the code: sc = dnnlib.SubmitConfig()\n",
      "\n",
      "Since this is coming from the dnnlib, which was written by the model's authors, not me, I'm not sure how to proceed. Any help would be greatly appreciated! :)\n",
      "How does hyper parameter needs change when we increase the model size? For example if we change the architecture from resnet50 to resnet152. Is there any trend that normally works like increase model size and increase lr or  weight decay or something? Thanks.\n",
      "What's the difference between nn.Embedding (pytorch) and vq-vae's codebook?  Are these two concepts the same idea? or does it make sense to implement codebook with the nn.Embedding class (pytorch)? I know nn.Embedding is used for a trainable lookup table. And codebook (in vq-vae) seems with the same idea to me (trainable, also a lookup table) so what's the difference? I can not clearly distinguish them. Thanks.\n",
      "I'm trying to use pre-trained models from Intel to do simple detection of objects passing by in the street. I've downloaded pretrained models from [https://docs.openvinotoolkit.org/latest/omz\\_models\\_intel\\_index.html](https://docs.openvinotoolkit.org/latest/omz_models_intel_index.html) . At this point I have an XML and a BIN-file with the model I would like to use. What I would like to do is to apply the model on a picture using a python program, but I am at a loss for how to import the model. Any ideas or guides on how to do this?\n",
      "Has there been any study on how to choose the relative weights for compatible objectives?  Especially concerning the effect this has on overfitting...\n",
      "**Would DeepMind Control Suite be enough for a publication?**\n",
      "\n",
      "It's rough as a PhD student in deep RL. Lots of industrial competition and ever-changing expectation/standards. So forgive the very general and informal question, but would the DeepMind Control Suite satisfy reviewers for a modern RL publication?\n",
      "\n",
      "Say, for a new general-purpose algorithm that achieves better performance than SAC, PPO, and A2C.\n",
      "\n",
      "Or if that’s too simple, which additional experiments (algorithms and environments) would be more ideal to test? There was a time not long ago when Atari was enough.\n",
      "Are energy based models primarily used to generate images, or can they be used for tabular data? For tabular data, how might it work differently from SMOTE?\n",
      "Hello everyone - this might be a very simple question so please feel free to point me in the right direction. \n",
      "\n",
      "I am a data science student who works as a business analyst for an insurance company. As a result, I use Excel in my day to day and R/Python at school.\n",
      "\n",
      " I was working on a fun project (solving a silly riddle) and used the Solver function in Excel to find my answer. I formatted my problem so that I effectively needed to find the optimal string of 27 values, where each value was a 1, 2 or 3. Despite the large number of permutations, Excel was able to find me the correct answer after testing some 5k combinations and did so rather quickly. \n",
      "\n",
      "I tried to do the same in R and found it was significantly more challenging, and when I did find the correct result, it took longer then in Excel. I was curious if I was using the wrong package or was otherwise confused.I set up the problem in the same way, then used a Genetic Algorithm package \"GA\" to find my solution. I was not able to find an algorithm that allowed for restricting inputs to integers, so I had to use the binary feature in this package and convert the binary string to my series of integers. \n",
      "\n",
      "This approach found a correct answer in only 32 attempts - but the run time was longer then in Excel. \n",
      "\n",
      "I guess my questions are (1) am I understanding the use of the R package correctly? Is there a better approach in R? (2) I was able to read about how GA works, however, I am vague on how this is different than the Excel evolutionary solver approach.\n",
      "I'm working with StyleGAN-encoder, using the code from this [jupyter notebook](https://github.com/Puzer/stylegan-encoder/blob/master/Play_with_latent_directions.ipynb).\n",
      "\n",
      "generator = Generator(Gs\\_network, batch\\_size=1, randomize\\_noise=False)\n",
      "\n",
      "yields the following error:\n",
      "\n",
      "[/tensorflow-1.15.2/python3.6/tensorflow\\_core/python/framework/ops.py](https://localhost:8080/#) in \\_as\\_graph\\_element\\_locked(self, obj, allow\\_tensor, allow\\_operation)    **3647**           raise KeyError(\"The name %s refers to a Tensor which does not \"    **3648** \"exist. The operation, %s, does not exist in the \" -> 3649                          \"graph.\" % (repr(name), repr(op\\_name)))    **3650** try:    **3651** return op.outputs\\[out\\_n\\]  KeyError: \"The name 'G\\_synthesis\\_1/\\_Run/concat:0' refers to a Tensor which does not exist. The operation, 'G\\_synthesis\\_1/\\_Run/concat', does not exist in the graph.\"\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Can anyone help me? thank you so much\n",
      "When you are adjusting the learning rate automatically, wouldn't you want the LR to be a decreasing function of the # of epochs? I'm working through Moroney's Tensorflow in Practice specialization and the function they use is \n",
      "\n",
      "`lr  = 1e-8 * 10 **(epoch / 20)`\n",
      "\n",
      "which is monotonically increasing as a function of the epoch\n",
      "Hey, If anyone experienced in time series can help it would be great.\n",
      "\n",
      "I am working on agriculture dataset, that contains 3 years state-wise data of agriculture farming on different crops, for ex  \n",
      "City  - 2016,         2017,        2018     crops  \n",
      "tokyo -5000ha , 2500ha,    3421ha  rice  \n",
      "Saitama -  5131ha, 3232ha, 2932ha  rice  \n",
      "tokyo - 2012ha,  312ha,    2479ha, peanuts  \n",
      "saitama - 213 ha, 892ha,  9832ha ,peanuts  \n",
      "\n",
      "\n",
      "I hope you get the idea, so How can I use time series to predict how it will be going, there are many features and I don't know how to go  \n",
      "As the data is private, I can't share the data, sorry for that!\n",
      "\n",
      "Thanks for help\n",
      "Hey everyone,\n",
      "\n",
      "I'm trying to use this loss function: [Weighted Hausdorff Loss](https://github.com/danielenricocahall/Keras-Weighted-Hausdorff-Distance-Loss) with this implementation of [U-Net](https://keras.io/examples/vision/oxford_pets_image_segmentation/#perpare-unet-xceptionstyle-model) .\n",
      "\n",
      "The loss seems to be written for TF 1.15, I'm currently using TF 2.4.0. The problem is, that I'm getting **no gradients** with this loss, when using it in training, but when I give it sample data, just evaluating the loss (forward pass) works flawless, so every function used should work with TF 2.4 or am I missing something here?\n",
      "\n",
      "Thanks in advance.\n",
      "Hello Everyone,\n",
      "\n",
      "I was wondering if someone can teach me more about Sammon Mapping?\n",
      "\n",
      "I understand it is a dimensional reduction technique, but is it in itself a clustering algorithm that can be considered an unsupervised learning method?\n",
      "\n",
      "Any insight into this mapping algorithm and how it relates to unsupervised learning would be very helpful.\n",
      "Hi everyone!\n",
      "\n",
      "I am an Austrian student, and I am looking for a real-world practitioner working with algorithms e.g., in social media or any other areas. Since it is very short-term and I have no idea where to find experts like you, I am posting here.\n",
      "\n",
      "I have to do a short interview on the subject of algorithms for an university course. The starting point was the article \"Human decisions and machine predictions\" from Kleinberg et al. \\[[https://academic.oup.com/qje/article/133/1/237/4095198\\]](https://academic.oup.com/qje/article/133/1/237/4095198]) (does not have to be read, I am preparing a short summary)The interview consists of \\~11 questions can be answered by e-mail. (It's not too specific and rather superficial)\n",
      "\n",
      "Thank you in advance for your time!\n",
      "Hi. \n",
      "\n",
      "Need to clear something up on scikit learn:\n",
      "\n",
      "Am I understanding correctly in that the `sklearn.model_selection.permutation_test_score` function performs the  y-randomization (y-scrambling) test of an ML model?\n",
      "Hey guys , happy new year ,\n",
      "And quick question , I learnt python and used it for some time , but I am facing difficulty when I have to use it in real time like mainly logical part of code\n",
      "\n",
      "Example - scrap an website using selenium.\n",
      "\n",
      "I can write the code where it goes to the page and does all selections and but when it comes to logical part then I am struck.\n",
      "\n",
      "Any suggestions and I want to apply python for machine learning , so any suggestions , how to overcome this ??????\n",
      "I wanted to know how to bring in kernel transformation to the K nearest neighbours algorithm. Any help is welcomed.\n",
      "I have implemented a transformer following Peter Bloem's blog that works well, but I am somewhat confused by the position embeddings. The material I am finding on position embeddings describes the embeddings as being with respect to a single sentence. But in Bloem's transformer model, the position embedding size is \\`(size\\_of\\_word\\_embedding, length\\_of\\_longest\\_sequence)\\` where the longest sequence has a length of 512, which surely includes multiple sentences. So the position embeddings here seem to be with respect to the entire sequence of sentences, not a single sentence. Accordingly, position 354 could be the first word of the sentence for one sample and the last word of the sentence for another sample. \n",
      "\n",
      "Am I interpreting this correctly? If so, the position embeddings seem meaningless to me. Knowing that \"cat\" is the 2nd word of a sentence vs the last word of the sentence seems meaningful. But there is no reason to believe that \"cat\" being at position 354 in sample a and also position 354 in sample b has any special meaning. How do we get a meaningful embedding when the position can mean anything?\n",
      "\n",
      "Or perhaps the usefulness of the position embedding is more within sample. So if we have a period at position 352 and \"cat\" shows up at position 354, that is different than \"cat\" being at position 353 and a period being at position 354. But then how do we learn an embedding that would be useful for the same position for other samples?\n",
      "Hi. I have been looking for articles that describe how I can expect the loss to vary when training a neural network. Obviously LR is very important. But can one e.g. say anything general about expected loss fluctuations when using decaying cyclical LR? Can one e.g. expect fluctuations in loss to only become smaller as LR cycles (and LR)  becomes smaller or is it completely problem dependent? My intuition is that loss fluctuations should become smaller as the max LR in a learning rate cycle becomes smaller. But I might be completely wrong? Does anybody know of good articles/papers that describe this subject?\n",
      "Are there any gift codes for Colab Pro? Want to buy Pro for a friend as a gift but can't find how to do it\n",
      "I am interested in learning numerical optimization from practical point of view. Is there any course (PhD level) with videos + lecture notes + solutions to assignments online?  \n",
      "I really liked  Ryan Tibshirani's course ([https://www.stat.cmu.edu/\\~ryantibs/convexopt-F15/](https://www.stat.cmu.edu/~ryantibs/convexopt-F15/)) but the solutions to assignment is not available.\n",
      "i want to start studying deep learning and neural networks. is knowing the math under the hood important, or simply beneficial? if needed, what is the correct math to learn, and how might one go about learning it with the topic of deep learning in mind?\n",
      "Does using a GPU help speed up neural network prediction (as opposed to fitting)?\n",
      "its possible to make a face generator with a pix2pix(encoder-decoder) model? what kind of input data to train I need?, gaussian nosie or the same face as the output?, a discriminative network + pix2pix model= Generative Adversarial Network?\n",
      "Hey everyone! There's an interesting problem I am facing with some regression data, any input appreciated!\n",
      "\n",
      "Assume that I am having a bunch of data points, and a regression scenario, meaning that I would like to predict a specific value of a test data point. However, the training data points I am given contain not only specific values for each data point, but also inequalities , e.g < 20 if we know that a data point has value y less than 20.\n",
      "\n",
      "The first obvious solution to that is to discard those points completely. Another solution would be maybe to adjust the loss function --let's assume MSE here for convenience -- so that, during training, if the model, for the specific data point mentioned above, predicts any value less than 20, the data point does not contribute in terms of loss. However, I would argue that this kind of an approach could harm the overall accuracy of the model, since any value below 20 will do (and many weight configurations can theoretically give minimum loss).\n",
      "\n",
      "Do you know any approach that is more robust to inequality data existing in a regression problem? Thanks in advance!\n",
      "Hi all, how would you do few-shot learning for text classification using transformers? Thanks!\n",
      "Can anyone suggest some machine learning paper they might have read recently? I am supposed to make an application utilizing the paper(in the field of cognitive computing) but I am having a hard time choosing a paper. So can anyone send any of their suggestions or an interesting paper that they have seen or you could make a cool project out of it?\n",
      "i am firmware/ bios developer but i m interested to get into data analysis with AI.  Can someone point me some paths or what i should start study/learning in order to go down this path?\n",
      "Hey fellas. I'm going to be working on some natural language processing classification models.\n",
      "\n",
      "What's the best way to deal with noise in my sample space? I'm going to be grabbing messages from chat rooms (sometimes the log will have over 100,000 comments), and moderators won't be able to catch every bannable offense. My interest is in classifying when a user is predicted to be banned.\n",
      "\n",
      "Also not sure if I should go for classification or regression\n",
      "Hi guys, I'm new to this and would like some advice.\n",
      "\n",
      "I'm trying to train a model to identify printing errors with two inputs. The first input will be a picture of the \"sample\" print, and the second one will be a picture of the \"new\" print. The program will tell if there are printing errors in the \"new\" print compared to the \"sample\" print.\n",
      "\n",
      "Is this something that a beginner will be able to do? If so please point out some resources that I can look into.\n",
      "\n",
      "Thanks a lot!\n",
      "Any reason why my accuracy is so low? I'm getting 10% on cifar10. If I make the relevant changes to the shape and and input size and apply it to mnist I get > 90%. I'm new to this but not sure why the results are so drastic between the 2 datasets.\n",
      "\n",
      "    from keras.datasets import cifar10, mnist\n",
      "    from keras.models import Sequential\n",
      "    from keras.layers.core import Dense, Activation\n",
      "    from keras.utils import np_utils\n",
      "    from tensorflow import keras\n",
      "    \n",
      "    (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
      "    \n",
      "    X_train = X_train.reshape(50000, 3072)\n",
      "    X_test = X_test.reshape(10000, 3072)\n",
      "    \n",
      "    classes = 10\n",
      "    Y_train = np_utils.to_categorical(Y_train, classes)\n",
      "    Y_test = np_utils.to_categorical(Y_test, classes)\n",
      "    \n",
      "    #Set hyperparameters\n",
      "    input_size = 3072\n",
      "    batch_size = 100\n",
      "    hidden_neurons = 100\n",
      "    epochs = 25\n",
      "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
      "    \n",
      "    #\n",
      "    model = Sequential()\n",
      "    model.add(Dense(3000, input_dim=input_size))\n",
      "    model.add(Activation('sigmoid'))\n",
      "    model.add(Dense(2000, input_dim=3000))\n",
      "    model.add(Activation('sigmoid'))\n",
      "    model.add(Dense(classes, input_dim=2000))\n",
      "    model.add(Activation('softmax'))\n",
      "    \n",
      "    #\n",
      "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
      "    \n",
      "    #\n",
      "    model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)\n",
      "    \n",
      "    #\n",
      "    score = model.evaluate(X_test, Y_test, verbose=1)\n",
      "    print('Test accuracy:', score[1])\n",
      "\n",
      "Any help is appreciated. Thank you.\n",
      "Hello ML Enthusiasts,  \n",
      "I have worked with javascript in the past but I have always been fascinated by ML. I never really had the guts to learn ML but now I want to do it. So as an absolute beginner how should I start learning ML. Right now I don't have any idea of how ML functions or what are the concepts that I should learn. I just don't know where to start.  \n",
      "\n",
      "\n",
      "I googled about how to start learning ML, but the opinion varies on learning by joining video classes or learning by books. As a beginner, I do not want to spend much money into this. So are there any free video courses that I could start watching or maybe free e-books. I thought the best place to ask was right here, where I would get an honest opinion from someone who has already dove deep enough in the ML world.  \n",
      "\n",
      "\n",
      "Thanks in advance. Any advise or opinions are welcomed.  \n",
      "\n",
      "\n",
      "Maybe after few years, I hope even I could answer the question regarding ML in this community :)\n",
      "I have a problem, could anyone offer me advice?\n",
      "\n",
      "The book Eikon Basilike has no known author, it could have been written by either Charles 1st, John Gauden or someone unkown. We have work written by Charles 1st (The Kings cabinet opened) and work written by John Gauden (A Religious and Loyal Protestation).\n",
      "\n",
      "How possible would it be to confirm who wrote the book Eikon Basilike?\n",
      "Hello! I am trying to run [stylegan-encoder](https://github.com/Puzer/stylegan-encoder) using their pre-written [jupyter notebook.](https://github.com/Puzer/stylegan-encoder/blob/master/Play_with_latent_directions.ipynb) I am using tensorflow 1.14 and have ensured that it is connected to a GPU on google colab.\n",
      "\n",
      "I have gotten the full notebook running but when I try to learn new vectors in the terminal, I am getting an error coming from the authors' encode\\_images.py\n",
      "\n",
      "**I am running in terminal:** python encode\\_images.py aligned\\_images/ generated\\_images/ latent\\_representations/\n",
      "\n",
      "**And getting this error:**  \n",
      "File \"/content/stylegan-encoder/encode\\_images.py\", line 73  \n",
      "img.save(os.path.join(args.generated\\_images\\_dir, f'{img\\_name}.png'), 'PNG')\n",
      "\n",
      "I have read that f'{} requires python3, but I cannot run python3 without tensorflow 2.x, while this model requires Tensorflow 1.14. I am not sure how to get past this.\n",
      "\n",
      "Can anyone please help me? Thank you!\n",
      "Hello! I am trying to find a face recognition algorithm in order to organize an image archive. Most of the photos are in low resolution. What do you suggest for this task? Thanks in advance!\n",
      "Hi, Im new to ML (literally just finished CS50 AI) and Im trying to build a Cat vs Dog Classifier with Tensorflow and Keras\n",
      "\n",
      "However, my validation loss and accuracy is very unstable and fluctuates between each epoch. Also, my validation accuracy seems to plateau very early on whereas my training accuracy continues to increase, resulting in a rather significant discrepancy between my training and validation metrics. Ive tried using Dropout between layers to reduce overfitting but my accuracy seem to cap at 85% for all the tests I've run.\n",
      "\n",
      "Accuracy: [https://imgur.com/gc1w835](https://imgur.com/gc1w835)\n",
      "\n",
      "Loss:  [https://imgur.com/BfWsXV9](https://imgur.com/BfWsXV9)\n",
      "\n",
      "My sample size is 13,000 for each class, using 30% for testing. My images are 100x100px, Grayscale, normalised to pixel values between 0 to 1.\n",
      "\n",
      "Below is my model architecture:\n",
      "\n",
      "[https://pastebin.com/bpgt66Gh](https://pastebin.com/bpgt66Gh)\n",
      "\n",
      "Is there any other way to improve my model? Thank you!\n",
      "Hi looking for some tips for an idea. Is it possible to make an ai picture search for pdf? For example, if I had a book about car engines and wanted to look for a part. I could type a part number and the search would take me to a picture of that part?\n",
      "\n",
      "Edit: please share if this has already been done thanks!\n",
      "Hi, I am looking for guides or books with info about **satellite imagery** segmentation and classification (**Tensorflow** is preferred). So far I found DeepSat V1 and DeepSat V2, but with my low exp it's not enough. \n",
      "\n",
      "Thanks for any resources!\n",
      "Hello. I know very little about ML, but was recently tasked with investigating the idea of using machine learning in AWS to predict how successful a production code release will be based on previous code releases. \n",
      "\n",
      "I'm looking to create a VERY simple proof of concept showing that this can be done. Any pointers of tools to use, methods to consider, pitfalls to avoid, etc. would be greatly appreciated! Thank you in advance\n",
      "Hello all,\n",
      "\n",
      "I am researching the problem of predicting monthly energy consumption and plan to compare SVR, CNN-LSTM hybrid, and RF's performance on it.\n",
      "\n",
      "The available data contains 4000 individual houses and is split into 5 datasets:\n",
      "\n",
      "* Time-series historical energy usage data\n",
      "* Time-series weather data (separate dataset for average, min and max)\n",
      "* Information on household and its occupants\n",
      "\n",
      "I've done a lot of machine learning projects before but have never used multiple different datasets before for a singular purpose.\n",
      "\n",
      "How would you utilise these multiple datasets in order to learn from them?\n",
      "\n",
      "My current thinking is to apply some unsupervised learning such as k-means to categorise each instance by household and climate type and then train a separate energy prediction model for each.\n",
      "\n",
      "Wondering what others' would do to approach this problem and if there is a better way to do this than the way I have proposed?\n",
      "I'm looking into RNNs and LSTMs and how they work. Behind the scenes, how do the shape of the input and previous hidden state change to produce the hidden state (and output)? \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Let's say I want to make a many to one LSTM without a library. I have the input with the shape \\[1, x, 6\\]. As far as I know, x is the number of times each LSTM cell runs and 6 is the number of things in the input (and 1 is the batch size). \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "How could I end up with an output of a different shape (let's say \\[1, 1, 4\\] or \\[1, 4\\])?\n",
      "Tensorboard Images are shown incredibly overcontrasted - is my network learning with the wrong colors?\n",
      "\n",
      "I am trying to get a training done with the tensorflow object detection branch:\n",
      "\n",
      "[https://github.com/tensorflow/models](https://github.com/tensorflow/models)\n",
      "\n",
      "following the instructions from this tutorial.\n",
      "\n",
      "[https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)\n",
      "\n",
      "Everything seems to be working fine and after labeling and creating tf records I am finally training my network (SSD-Resnet50 on 640x640). After training, however, the quality of detections is rather poor, although I am (for now) only training with 1 class, I have sufficient images (750) and training in tensorboard looked as usual. However, when I switch to the Images-Tab in tensorboard and preview the images from the current batch, they are always contrasted too high, such that nothing is recognizable anymore.\n",
      "\n",
      "I was trying to find an answer if it might be preprocessing (which I dont, I am only using vertical and horizontal flip), but couldn't find anything.\n",
      "\n",
      "I thought, that maybe tensorflow is not able to read the images correctly (I have had my fair share of training with bgr-images from open-cv and then testing with rgb-images from PIL, wondering why the results were okish, but never good), but since I am using tfrecords and standard jpg imgaes, I would rule this problem out, aswell.\n",
      "\n",
      "Another suggestion I found was that it is a Tensorboard problem and that maybe updating would help, since I was using Tensorflow 2.2.0 (from the Tutorial), but also Tensorboard 2.4.0 didn't change the problem.\n",
      "\n",
      "At this point, I am out of solutions and would like to know, if there is anyone else that has (had) the same problem and maybe knows, how to fix it?\n",
      "\n",
      "Also: if my network is training with these corrupted images, the quality of detections can't be that good, which would further motivate me to get rid of this problem.\n",
      "Hello.  \n",
      "I am trying to see, if a problem could be solved with ML. The problem could be simplified that way:\n",
      "\n",
      ">Imagine a variant of scrabble where certain letters have special rules (eg. at the beginning of the game, you draw a letter and you can reuse that letter every turn). Given a set of letters (and their specific kind), determine if that hand is potentially high-scoring.\n",
      "\n",
      "From what I have read, a simple transformer encoder would be a good solution. Instead of position embedding, we can use \"kind embedding\". The position-independent nature of self-attention also seems appropriate.\n",
      "\n",
      "I'm curious to see, if there is anything I could have missed.\n",
      "Hi.  \n",
      "I am currently studying ML via: \"Hands-On Machine Learning  by Aurélien Géron\" and DataCamp courses, but I don't feel like I am doing any progress. Do you think  that I shoud start trying out kaggle competitions, or any other form of practice? Or just finish the book first and then try kaggle?\n",
      "Is it better to normalize all my data by the same factor or normalize each feature/column separately.\n",
      "\n",
      "Example: I am doing a stock prediction model that takes in price and volume. A stock like Apple has millions of shares traded per day while the price is in the hundreds. So normalizing my entire dataset would still make the price values incredibly small compared to volume. Does that matter? Or should I normalize each column separately?\n",
      "Is it good idea to learn machine learning move to Math major from the Informational technologies?\n",
      "Hi.\n",
      "\n",
      "I'm 21 and currently studying Mechanical Engineering. I'm planning learn ML and build my  career on it. As a Mechanical Engineering student, can I find a job related to ML after I graduate from university ?\n",
      "Does anyone have some recommended resources for data prep cookbooks on predictive churn modeling? Ideally covering use case of starting with customer logs and getting to finalized summary tables, with advice for partitioning train/validate/test, as well as preventing the same customer from appearing in multiple partitions, etc.\n",
      "I have a minor question: You see all these youtubers making AI to, for example, race cars down a track. And the AI generally gets pretty good at it. However, what rarely seems to be demonstrated is, do these AI actually learn how to race on a track, or how to most efficiently complete that one specific track?\n",
      "\n",
      "If you put them to a similar task but with a different environment, would they use their previous \"experience\" to not just bumble around for the first 100 generations?\n",
      "I am trying to understand variational autoencoders on the MNIST dataset with very little background in probability and statistics.\n",
      "\n",
      "1) I don't understand the need to marginalize over the latent variable z when computing p(x|z)p(z). In the code, p(z) is first computed by sampling from a normal distribution before it is sent through a neural network to generate the image i.e. compute p(x|z). Why don't we need to marginalize over the latent variable z here ?\n",
      "Does anybody know of any chatbots that are open sourced and you can download directly to your computer and text it’s for a project like a discord bot but it doesn’t have to be discord\n",
      "[deleted]\n",
      "This is a question I came across in a ML course I'm taking.\n",
      "\n",
      "`Suppose we generate a training set from a decision tree and then apply decision-tree learning to that training set. Is it the case that the learning algorithm will eventually return the correct tree as the training-set size goes to infinity? Why or why not?`\n",
      "\n",
      "I think that the accuracy of the new decision tree we are building will be bounded by that of the original decision tree. So it won't necessarily be the correct decision tree as it will stick to the trends it sees in the first decision tree we are pulling the training data from.(no matter how much training data we generate and present)\n",
      "\n",
      "I would like to know your idea on this.Thanks for your time.\n",
      "I'd like to have your thoughts about an approach that I'm currently trying in my job. \n",
      "\n",
      "My objective is to understand client classification and identify the most important drivers (**features**). Hence I am not interested into predicting anything for the moment.\n",
      "\n",
      "I know that the common way to do this would be with unsupervised learning. However, I would prefer to not go with this method for three reasons: \n",
      "\n",
      "* I am not a big fan of unsupervised learning as the accuracy is more difficult to measure \n",
      "* I have access to the contract type of each client so I’d like to use it as label \n",
      "* I’d like to explain my results, and for this task there are some quite useful libraries for supervised learning (SHAP, LIME) that I have experience with \n",
      "\n",
      "Since I’d like to understand patterns and **not** predict anything at the moment, I am more interested in the training phase. Let’s say I use a basic Decision Tree. My idea is to fit the model to the data and look at the most important features. \n",
      "\n",
      "Here is my question: \n",
      "\n",
      "For such a task, is it ok to evaluate a model on the training data themselves? I know this is usually quite a no-go because we want to generalize the algorithm the best as possible, but I believe here it makes sense to do so here.\n",
      "I would like to do a regression Task (find x,y position of an Object in some images). Therefore I have about 2000 annotated training images (I can annotate more maybe up to 20000). My first thought was to use CNN's but the size of my input data is not consistent and I don't want to lose feature by resizing or cropping images. What do you think? Is there a better way than using CNN's? Maybe  FCNN ?\n",
      "\n",
      "ps. Image sizes are aprox 120 x 50 pixel\n",
      "Why one-hot vectors is considered a non-sparse data type? I am not from a data science background. From my perspective, the representations of one-hot vectors contain a lot of 0's inside, thus should very well belong to the sparse data type. Please correct me if you understand why.\n",
      "Need help!!! Looking for Anyone who has experience in fast neutral style transfer\n",
      "Hi all, \n",
      "\n",
      "I'm building a DL rig for a student organization and I'm wondering **how to share it with students**. I want to be able to **create VMs and erase/reconfigure them** if students mess up. How would I go about doing that ?\n",
      "\n",
      "Thanks !\n",
      "How do you know if you're right when carrying out classifications? I just completed a Machine Learning class and achieved a pretty good grade overall despite genuinely not having a clue as to what I was doing. \n",
      "\n",
      "For example in an assignment that we had, the task was to classify the prevalence rate of diabetes within a community based on the shopping habits of the community. Some classifications which were used included KNN and Gaussian Naive Bayes and I understand what these do but what do their results actually mean? \n",
      "\n",
      "Lets take KNN. I understand that kNN classifies based on the distance between a query and its neighbours but if I obtain its accuracy I get a random percentage lets call it 'x'. What is 'x'??? Or say I obtain its F1-Measure, I get another percentage which is based on the models precision and recall but how is any of this actually useful? It really just seems like you're applying functions to datasets and then making an educated guess as to if they're correct or not. \n",
      "\n",
      "Furthermore, how do you even test if your model is right? If I take the diabetes example, do I validate my model based on other shopping datasets? And if I do this, why on earth would I trust given correlation doesn't imply causation.\n",
      "Thoughts on a necessity of a second course on Linear Algebra?  I've finished quite a detailed study of a first course, up to SVD and all applications in between (vector spaces, eigenvalues, orthogonality, similarity, inner product spaces, etc... - Something I've done in the past, but it's worth refreshing).  Is it worth continuing on?  I'm thinking my time is better spent moving into a more detailed study of probability at this point in terms of basic math.\n",
      "Looking for architecture recommendations for time series prediction. My dataset has 5 input variables and looking to predict one variable. Happy to continue discussing. Thanks :)\n",
      "Imagine a math app for elementary school students. Each math problem in the database has a tag (e.g., simple addition, long division). In answering x amount of question sa day, the ML model identifies problem areas and gives the appropriately tagged problems so that students can zero in on that particular tag/skill.\n",
      "\n",
      "What ML model would this be? A recommendation system?\n",
      "\n",
      "And is there some already existing, available code I could view to see how it would look in the education context (instead of Netflix) somewhere on GitHub?\n",
      "How does task that aren't Image-to-image translation work with Pix2pix?\n",
      "\n",
      "zi2zi, a Chinese alphabet generating [GAN](https://github.com/kaonashi-tyc/zi2zi) uses pix2pix for generating images. I also have seen many other applications using pix2pix for tasks that aren't related to image-to image translation. I compared the code of zi2zi with regular pix2pix, and found some code that I can't understand. \n",
      "\n",
      "1. What is the target source and where is the random noise? Unlike image-to-image translation tasks where there exists an obvious target image, what is supposed to be the target source for character generation? \n",
      "2. Suppose the output of the encoder portion of the unet is the latent space, then how are we supposed to set the latent space to a certain value for evaluation, exploration of the latent space while the decoder is effected by skip-connections of the encoder network?\n",
      "What is the best conditional gan that you can fine tune with your own data?\n",
      "After applying K-Means clustering on unlabelled data, why would you run a  linear classifier using as pseudo labels the cluster assignments? And What type of regularisation would you use and why?\n",
      "I think this has probably been asked before, but I tried looking on subreddit search box for \"project sources\" but I nothing came up that fit what I was searching for. I'm trying to do the Google TF Certification, and I thought it'd be best to learn by doing instead of just watching Coursera/Udacity videos and doing their exercises.\n",
      "\n",
      "Basically once upon a time, I think I had surfed the web and came across a book that resembled O'Reilly's [\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) but it was particularly for something like, \"15 Projects to start with Machine Learning\" which of course if one googles that particular keywords, there's PLENTY to choose from, but nothing like a tutorial, just ideas. Like [this](https://data-flair.training/blogs/machine-learning-project-ideas/) or [this](https://www.kdnuggets.com/2020/11/greatlearning-ai-project-ideas-beginners.html)\n",
      "\n",
      "I'm not new to Python, but still just dipping my toes on Machine Learning. I've not made nor transfer learned any models whatsoever, but I learn best when I'm confronted with a project or in particular, a problem and then try to find a way to solve it. \n",
      "\n",
      "**tl;dr:** So my question is, do you guys have used any particular source to do the kind of \"list of hobby project\" that I mentioned above and can you share the source to me? Or even better, know the book that I was talking about, and prove that I didn't daydream that book? :D Thanks!\n",
      "How do systems like DeepMind's MuZero know whether they've lost a game of Atari?\n",
      "What are the dimensionality of all the variables used in the K-means algorithm?\n",
      "I’m pretty new to ML and AI application. I’ve done a subject back at uni about the theory but this is a whole other ballpark.. I’m currently trying to make a NN with Node.js, and how do I know if the config i’ve set for a certain NN will eventually learn what I’m trying to accomplish or just fail? Thanks!\n",
      " Why are the neural networks weights initialised with random values?\n",
      " How can one decide whether the network is overfitting or underfitting the data?, What is the process that we need to follow.\n",
      "When is your dataset suitable for K-means clustering?\n",
      "So, far I have only seen that metric learning is applied to image data. It means we don't need metric learning for tabular data?\n",
      "I'm not a ML developer/researcher. I'm thinking of taking some courses later down the line, but I've got plenty of python experience and can navigate my way around existing code given enough time/effort.\n",
      "\n",
      "With that in mind, I'm currently looking for some code which can generate an artificial face, and then create a set of images of that same face from different angles and with different expressions.\n",
      "\n",
      "I'm sure there's something like this already out in the wild. Could someone point me in the right direction?\n",
      "\n",
      "Thanks!\n",
      "I have a very general question. My impression of the current state of AI is that we have found a method (neural network) that work very well with image and pretty well for text and numbers and that kind of it. if you can shoehorn your problem into an image, like go or protein folding, you can use NN to solve it. if you can't like music then your kind of out of luck.  How far from the truth that assessment is ?\n",
      "I am trying to apply a NN to create an AI for a card game. For the input layer, I have features like (1) the card in your hand, (2) the cards around you that are known via trading, (3) number of players, etc.\n",
      "\n",
      "My issue is that I want the NN to decide whether or not to pass the card in its hand, but if I make this the output of the NN, then there is no feedback (such as winning or losing the hand) indicating whether this was a good choice.\n",
      "\n",
      "After typing this, it seems that the best route may be to have the output be the machine's confidence in winning with their current card, and if that confidence is below some threshold then it will pass the card?\n",
      "\n",
      "Thoughts?\n",
      "\n",
      "(p.s. the only experience I have with machine learning is Andy Ng's course on coursera)\n",
      "I have a question about classification using neural networks. Let's take a simple example of three classes classficiation and a neural network with 3 layers (one input layer, one hidden layer and one output layer). Naturally, the output layer consists of three activation nodes, the one with the highest output result determines the classe in which the input belongs. \n",
      "\n",
      "Why is the sum of the results of the three output units isn't equal to 1 ? Naturally the result of each of these units is the probability that the input belongs to a class, and since the classes are disjoint and they \"make up\" the universe of outcomes, the probabilities must sum to 1, I'm I wrong ?\n",
      "\n",
      "I hope someone can help me understand why it's not the case. Feel free to explain in advanced mathematical terms or redirect me to website/videos... that prove why it's not the case.\n",
      "In several cases, the covariance matrix is singular and cannot be inverted. In which cases is this more likely to happen? Why is this a problem?\n",
      "Looking to get into ML.  Have a 2yr degree in programming thats done me well and have experience in python, c++ and C#.  Any good recommendations on where to start?\n",
      "What are the limitations and assumptions of the K-means algorithm?\n",
      "\n",
      "What is the time complexity of the K-Means algorithm, expressing it as a function of its inputs such as the number of clusters and/or the number of data points?\n",
      "There are no assumptions of kmeans \n",
      "Limitation : not suitable for high dimension data.\n",
      "K-means does not initialize random centroid far from each other ( K means ++ solve this problem )\n",
      "[deleted]\n",
      "Can anyone suggest me a deep learning based object detection model that is best for detecting objects on webpages?\n",
      "been trying vision transformer, can we put something (layers) before or after the transformer?? test accuracy is rubbish but train is somehow good\n",
      " I've been looking at the problem of *representation learning*, and I'm trying to reformulate the different types of learning problems to make representations appear explicitly.\n",
      "\n",
      "We can typically see the following in the literature (with *x* the input, and *y* the target/class):\n",
      "\n",
      "* Supervised Discriminative Learning: *p(y|x)*\n",
      "* Supervised Generative Learning: *p(x|y)*\n",
      "* Unsupervised Discriminative Learning: *p(g(x)|x)*\n",
      "* Unsupervised Generative Learning: *p(x)*\n",
      "\n",
      "As I was saying, I'd like to make \\*representations\\* appear explicitly in those formulations. By representations I mean the last set of features produced by a network's backbone, and that can be used for transfer to downstream tasks. Staying generic, I denote these representations *f(x)*, and as a consequence came up with the following formulations:\n",
      "\n",
      "* Supervised Discriminative Learning: *p(y|f(x))*\n",
      "* Supervised Generative Learning: *p(x, f(x)|y)*\n",
      "* Unsupervised Discriminative Learning: *p(g(x)|f(x))*\n",
      "* Unsupervised Generative Learning: *p(x, f(x))*\n",
      "\n",
      "I wonder what you think about it, because I'm not 100% convinced myself! For instance, I'm not entirely sure if *x* should still appear for the discriminative approaches (i.e. *p(y|f(x),x)* and *p(g(x)|f(x), x)* instead), as the representations already depend en *x*. Likewise, I'm not sure if the representations should be part of the joint or the condition for generative approaches (i.e. *p(x|f(x),y)* and *p(x|f(x))* instead). I could see how both could be rationalized.\n",
      "\n",
      "What do you think?\n",
      "I have been doing biomedical image segmentation on brain MRI images and basically I have been using 2D and 3D unets which is the most dominant architecture in this field.\n",
      "\n",
      "I am keen to explore other model architectures and maybe there are things people have done in the computer vision field which may be translated to biomedical image segmentation. Looking for suggestions and potential interesting things to try.\n",
      "For the MS COCO dataset format, what is the area property supposed to represent? I am under the impression that it is the area of the segmentation masks if there are any, but the framework that I am using thinks that it is for the area of the mask, and if there is no mask then it should be the area of the bounding box.\n",
      "I've been playing with VAE-variants, and am wondering if there are related works on the implication of training the encoder and the decoder networks with different optimizers, eg. Adam with lr\\_enc for the encoder and SGD with lr\\_dec for the decoder.  I'd love to hear more about other's experiences on this kind of training or related works. Thank you!\n",
      "How is \"Next Gen Stats powered by AWS\" in the NFL implemented in machine learning?\n",
      "I'm trying to train image segmentation model with transfer learning using [https://github.com/qubvel/segmentation\\_models/](https://github.com/qubvel/segmentation_models/).\n",
      "\n",
      "1. Is there anything specific to take in account for constructing single class (background + target shape) image dataset from this task? I currently have \\~250 images with 1-4 target shapes labeled in them.\n",
      "2. Should dataset include images with only background? Is class imbalance something is should worry about here?\n",
      "3. I'm currently using dice loss + binary focal loss combination. Is it overkill for just background and single target class?\n",
      "Is there a site, database or something similar where I can download .pkl pretrained models for stylegan2? I’ve looked everywhere and everything I can find is the same 10 ones in a GitHub post\n",
      "Anyone heard about admission at MILA?\n",
      "I'm working in image classification. Someone mentioned that I could try to do input recovery from a tensor formed from a given layer of the DNN. In other words, can I recover the original test image from a tensor produced at the output of a given layer?\n",
      "Im looking to get started on a simple revenue projection model broken down by days and 15 minute intervals. The inputs would be historical revenue generated, weather data, promotion data, etc.\n",
      "\n",
      "Where do you suggest I get started? I'd like to try something out small scale, free if possible as a proof of concept. Thanks.\n",
      "I have a project deadline coming up, and although my model works, idk how accurate it is\n",
      "\n",
      "i have these three numbers: `Mean Absolute Error: 3.4079000949583915 Mean Squared Error: 16.84073524874502 Root Mean Squared Error: 4.103746489336911` but I'm not sure what they mean, whether its good or bad, or how to fix it\n",
      "\n",
      "[https://colab.research.google.com/drive/1Sr\\_0zuhNzwLr-j2HuMa43xhFnz1euTHX?usp=sharing](https://colab.research.google.com/drive/1Sr_0zuhNzwLr-j2HuMa43xhFnz1euTHX?usp=sharing)\n",
      "\n",
      "thats the notebook \\^\n",
      "\n",
      "ty for any help in advance\n",
      "I am training object detection models weekly that need to do predictions on millions of images that are stored on data servers. During testing this becomes a great bottleneck and increases testing time immensely. The images are currently stored in the png format. One method I could think of was converting them to jpeg and store locally but that still would be not very efficient. Is there any hashing method or any other compression strategy that would allow for storage of the images locally to speed up the testing time?\n",
      "Hi everyone, SUPER new to this. I sometimes have a hard time learning if I don't understand the big-picture concrete ideas that go along with the practical execution of concepts, so I would love someone to guide my thinking a little bit. So this is more of a \"tell me in which ways I'm right and which ways I'm wrong\" situation.\n",
      "\n",
      "My initial understanding is that creating a Machine learning model is essentially being given (or creating) a dataframe (dataframe1). And then using the information given in that dataframe to create a new dataframe (dataframe2) with some of the data from dataframe 1, but also some new data that comes from your human understanding of the data in dataframe1 and how you can create new data that appropriately defines the relationship between two variables. Sometimes you leave out data from the original dataframe that isn't important (for example, free throw percentage might not be as important to the flow of a predicted basketball game, as it's isolated, but the amount of free throw attempts might be relevant). \n",
      "\n",
      "So my understanding is that my ability to set aside irrelevant information and create new, relevant information (based on original  data) will be the things that determine the strength and accuracy of my model.\n",
      "\n",
      "So machine learning (in practice) seems like it's essentially creating a new, more efficient collection of data based on your exploration and understanding of the data provided, Testing and Trial-and-erroring your assumptions along the way?\n",
      "\n",
      "Is that even close to being right? I know obviously there's a lot more to consider and think about and learn, but any clarification with this massive, concrete idea would really help my ability to direct my learning!\n",
      "I want to get involved in open source Machine Learning projects, i.e. building software for new algorithms, etc. Does anyone have any sources that I can look into?\n",
      "Not sure if this is the right place for this but I'll give it a shot. I'm using tensorflow and recently my convolutional networks have been running about 10 times slower than usual. I tried running older code to see if it was a change i made but that ran slowly too. Plain fully connected networks run fine. \n",
      "\n",
      "I tried contacting the university's administrator (I'm running models on a university server) and they said that they changed the version of Cuda over December from 11.0 to 11.2. Apparently the older versions are still installed so I could theoretically tell the program to use a different version, but I don't know how. Would I change it in the program or in my virtual environment or somewhere else? \n",
      "\n",
      "Also does anyone have any other ideas why my models would be running slower than usual?\n",
      "**Feature Importance in Boosting vs Bagging Methods**\n",
      "\n",
      "I am currently working to understand which of the metrics I am collecting are most informative for both regression and classification problems using Feature Importance. The datasets I am working with are small (N\\~50-200) and I am getting similar performance with Random Forest and XGBoost, but I noticed that the Feature Importance scores are very different between the two models. \n",
      "\n",
      "I am concerned that the higher weight assigned to elements that are more difficult to predict and to learners with better accuracy in boosting models will assign a higher Gini score to features that are better at predicting outliers (elements that are more difficult to predict) in the dataset. \n",
      "\n",
      "Since I am interested in the features that are informative for the majority of elements, is it better that I use a bagging method instead of boosting?\n",
      "https://trends.google.com/trends/explore?date=all&q=pytorch,tensorflow\n",
      "\n",
      "Why is the trend for PyTorch and TensorFlow going down? Are there new upcoming libraries or what is the reason? I am new to Machine Learning.\n",
      "Hey, i hope some smart people can answer this question! I'm trying to understand supervised machine learning in diagnosing exacerbations in COPD patients.  Usually the input variables or parameters are oxygen saturation, respiration rate etc (physiological parameters).\n",
      "\n",
      "How can one ensure that the predictor values / input values are correlated to the output? (exacerbations). I can't really understand how one can predict an exacerbation, without being 100% sure that for an example -> oxygen saturation is correlated with an exacerbation...\n",
      "\n",
      "Doesn't machine learning work by a human labeling the data, like \"oxygen saturation of \\*some random value\\* is labeled as an exacerbation? Or does the machine learning process figure out the value itself?\n",
      "Hi! \n",
      "I’m stumped on how to do this thing with error estimation. I feel so out of my element and no matter what I try I can’t seem to get a good estimation of how well or badly my model is predicting a specific element. \n",
      "I have a regressor model predicting a property for different elements of a population.  I was hoping I could cluster the population and have similar model errors within each cluster, and use that to estimate error for a given element of the population, but I cannot find a good clustering method for it. \n",
      "\n",
      "Are there other ways to do error estimation? I’d like to be able to report x+-y with the y value not being huge for each element (and have a different y value for each element, as appropriate) \n",
      "\n",
      "Thank you so much everyone!!!\n",
      "I have a multi-class classification problem in which one of the classes I am trying to predict is very sensitive to variations of the input data. Additionally, the input data has a very wide range of values. \n",
      "\n",
      "For simplicity we can assume I have one feature X that ranges from 0 to 500, and three classes, A, B, and C. Samples of class A are mostly located in 0 < x < 0.1, class B is mostly in 0.1 < x < 350, and class C mostly in 350< x < 500. Which ML models would do well and which would struggle with this type of problem in which class A is located in a tiny fraction of the feature space compared to B and C?\n",
      "\n",
      "Thank you in advance!\n",
      "Why does a vertical convolution pattern recognize horizontal lines?\n",
      "\n",
      "I was following the machine learning tutorial on [google codelabs](https://developers.google.com/codelabs/tensorflow-lab3-convolutions#3) and they defined a convolutional filter as :\n",
      "\n",
      "```\n",
      "filter = [\n",
      "\n",
      "    [-1, 0, 1],\n",
      "\n",
      "    [-2, 0, 2],\n",
      "\n",
      "    [-1, 0, 1]\n",
      "\n",
      "]\n",
      "```\n",
      "\n",
      "All the middle terms are zeroes so it seems as if this detects vertical edges, but the result had horizontal edges highlighted.\n",
      "\n",
      "Also it doesn't seem like they transposed it because this is how the result was calculated:\n",
      "\n",
      "```\n",
      "for x in range(1,size_x-1):\n",
      "\n",
      "  for y in range(1,size_y-1):\n",
      "\n",
      "      output_pixel = 0.0\n",
      "\n",
      "      output_pixel = output_pixel + (i[x - 1, y-1] * filter[0][0])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x, y-1] * filter[0][1])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x + 1, y-1] * filter[0][2])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x-1, y] * filter[1][0])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x, y] * filter[1][1])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x+1, y] * filter[1][2])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x-1, y+1] * filter[2][0])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x, y+1] * filter[2][1])\n",
      "\n",
      "      output_pixel = output_pixel + (i[x+1, y+1] * filter[2][2])\n",
      "\n",
      "      output_pixel = output_pixel * weight\n",
      "\n",
      "      if(output_pixel<0):\n",
      "\n",
      "        output_pixel=0\n",
      "\n",
      "      if(output_pixel>255):\n",
      "\n",
      "        output_pixel=255\n",
      "\n",
      "      i_transformed[x, y] = output_pixel\n",
      "```\n",
      "Is studying probabilistic machine learning and/or computational statistics a good time investment?\n",
      "How do yall deal with different sampling rates in your data?\n",
      "\n",
      "One obvious solution is to resample them to be the same rate, for example, if you have 500Hz and 1kHz in your data, you could just downsample all data to 500Hz. However, when parts of your data's sampling rates have uneven multiples of each other, for example, 512Hz and 1kHz, resampling requires your own interpolation (your choice of linear, cubic, ...). This method works to some degree for me, but I'm curious whether any of you have heard about/done any work on sampling-rate-agnostic architectures? \n",
      "\n",
      "My specific use case is in 1D sensor readings, but I think work in computer vision should also be helpful.\n",
      "[text data classification question] I'm new to AI ML. I'm writing a web app for my business that automates entering bills into quickbooks in order to reduce as much of the manual keyboard entry typically required for this task. Part of the needed automation is assigning an account to each line item on a bill. Right now I look at the contents of the bill line item and make the proper account selection. Example: a line item that contains the text 'Ranger Pro' should map to a COGS account called 'Herbicide'. What should I read to help me determine best classification method?\n",
      "I'm very new to ML and my project involves using ML to find the 'optimum' point to connect a set of three coordinate points. The model will be used for extending branches of a tree-like structure and I have an initial 'tree' to start with.\n",
      "\n",
      "The current model-based algorithm iterates and calculates values such as radius, volume, connection angle of the current structure. \n",
      "\n",
      "My question is, what is the most appropriate ML model for this application? I've looked at GANs but they are imaged based and don't seem to be applicable for extending off of existing data (in this case an initial tree with some branches).\n",
      "I'm currently working on a project in uni where I have to create a model that sorts pictures depending on if they contain sea landscapes or not.\n",
      "\n",
      "A basic intuitive idea was that the features were just the colour histogram of the given picture, but I'm pretty sure it won't be enough.\n",
      "\n",
      "I would like to know how I can count geometrical patterns in a picture, or even if I take some pictures of waves and count on the given picture how many waves.\n",
      "What do you all think some essential ML theory concepts are? Such as bias variance trade off.\n",
      "Hi, object detection noobie here. I want to detect objects using rotated bounding boxes. I labelled them with roLabelImg (rotated boxes version of LabelImg), but I can't feed that data into regular object detection models because there is one more variable (angle) in the annotation file. Is there a way to accomplish this task?\n",
      "Can someone recommend a good paper which I can try to replicate for learning purpose? I am looking for sentiment analysis's use in finance possibly using deep learning models.\n",
      "Hi, just a quick question. When employing expectation-maximisation to improve linear Gaussian mixed models, is it possible (and often) that you won't be able to calculate the log-likelihood as the determinants of some of your matrices (e.g. the covariance matrices of state transition and observation) turn up negative?\n",
      "\n",
      "I'm a beginner in control systems and wrote a bit of code for Kalman filtering and smoothing which works, but I messed up somewhere in the EM algorithm. I couldn't output a log-likelihood beyond the first few iterations as the term in my np.log became negative. Anyway, my log-likelihood kept decreasing lol, so I'm not sure if these two issues are separate or related.\n",
      "\n",
      "Appreciate the help!\n",
      "I have some user-submitted tags that look something like \\[\"Self-hosted\",\"Slaf-hosted\" ... \"self hosted\", \"proprietary\"\\]. I'd like to group them and use an indicator of group membership in a model. Some of the misspellings are kind of gnarly and most of the words are not common. What's a good model for this? Google Universal Sentence Encoder is a couple of years old now and takes a few seconds to return results in REST API mode. Is there something more lightweight or effective that can deal with unseen / misspelled words?\n",
      " \n",
      "\n",
      "**Why is overfitting/ Train on train with Recommenders/MF ok?**\n",
      "\n",
      "In most works with recommender systems (e.g. Matrix factorization models like SVD, ALS, DL + dot product/embedding models etc), the accepted approach seems to be to train a model on the data then get predictions on that same data, for purposes of model evaluation. i.e train a model on the train data, and get predictions from it. This is most common in cases where there's a \"2 step approach\", e.g. a candidate model and a subsequent deeper/slower ranking model (e.g. the Youtube recc model, https://www.tensorflow.org/recommenders/examples/basic\\_ranking etc).\n",
      "\n",
      "There doesn't seem to be much \"support\" as a best practice for doing, say, cross validated predictions on the underlying train data, before training the subsequent model(s) on its output.\n",
      "\n",
      "I understand that MF models \"overfit\"/memorize less than, say, decision trees, but this still seems very odd. You're still training the subsequent model on a different distribution (train on train) than the \"evaluation\" data.\n",
      "\n",
      "Is this just common overfitting, or is there some logic to it?\n",
      "So I have this classification project, you have set of coordinates for each planet relative to earth and based on that we classify, now my question is what would the feature vector look like? since we have 8 planets and each has 3 coordinates, do we concat them in one 1*24 vector or are there any different methods?\n",
      "Hi there,\n",
      "\n",
      "What is going on in the **variables** tab, and why does the tensor data type have so many layers?  (link [here](https://ibb.co/SncSv7k))\n",
      "Hi, is there any research on using images from different datasets for training on a smaller dataset? \n",
      "\n",
      "E.g. I have a class \"dog\" in my custom dataset that only has three images of dogs so I want to augment my training with images from a larger dataset. Is there a way to identify \"good\" images for training?\n",
      "I'm trying to learn English. is there any \"spell checker\" that is able to detect errors like: \"my telephone number is sorry\"?\n",
      "\n",
      "I tried many spell checker but no one is able to dect theese type of errors. Thanks\n",
      "Hi everyone, I am working on a machine learning problem where I have a small dataset of around 1500 examples and working on predicting congestion in IOT networks, is there any optimizer or algorithm or any exisiting dataset which can help me improve accuracy, for now I am achieving 85% and looking to get atleast 90-92%. Any help/leads would be highly helpful.\n",
      "Thanks!\n",
      " Is Nonlinear Gradient Temporal-Difference Learning only for on-policy evaluation? Because there seem no  Importance-weighting terms in the algorithms.\n",
      "Hi, I am an EE student and I am very interested in learning ML; i've been investing time to self-teach myself the concepts and fundamentals of ML. Do you guys have any BEST books recommendation for Maths in Machine Learning?? i'd appreciate it a lot.\n",
      "\n",
      "(note: you can assume I'm a pure beginner in the ML field)\n",
      "Are there any guides that take you step by step into moving objects detection in videos ? Especially I want to know how to convert videos into numbers (I've always worked on well-processed data so I always had matrices of numbers and I just used them) and how to use this data to detect objects... And also if there are any existing libraries that do the job well.\n",
      "\n",
      "Thank you by advance.\n",
      "Anyone knows good transfer learning book with example on how to do my own training, without having to deal with hyper-parameters, custom configuration or confusion matrices?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I have university covid project, in which I need to recognize whether person has medical mask on.\n",
      "\n",
      "The professor wants me to find transfer learning book (not online tutorial, because he feels like book is more credible).\n",
      "\n",
      "But he wants me to do it in brain-dead way: find already trained model (so the model training is fast, and so I don't need to configure) and just feed it my own dataset, using the configuration/schema of the model given in the book.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Anyone has any idea of such a book, which has trained model and only feed it a dataset of people with and without masks (my dataset is labelled)?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Any help is GREATLY appreciated.\n",
      "Is there any advantage in quantizing image inputs? For example, binarized images are sometimes used in mnist training and other character classification problems.\n",
      "\n",
      "Is there any relevant research on any usefulness of using lesser color levels for color images? Like using images with 64 color values as opposed to the typical 256, or something.\n",
      "So I am currently watching lessons from Machine Learning course by Andrew NG in Coursera, in week 6 he first talks about splitting the data set into 2 parts where these two parts are training set and test set, then selects the best fitting hypothesis function according to the error rate he got on these different functions.   \n",
      "After this video, he talks about Cross Validation Set. Where now he splits the data set into 3 parts where there are now Training Set, Cross Validation Set and Test Set. He then explains that it is better to use error rates that got by Cross Validation Set, but I wasn't able to get the idea that why it is better to select hypothesis function using the error rates that we got by Cross Validation Set. \n",
      "\n",
      "I tried to search about it but I think since the Cross Validation Set I learned in the course is very simple I got confused by the extra terms (like k-folding etc). Can someone help me to understand why it is better over just using two sets (training and test) ?\n",
      "Would it be possible to make algorithm that would take pirated camrip video file and corrected it to some almost original-looking version? Could you train it just with some original footage from trailers?\n",
      "Like you said, the linear single-layer case is PCA because both minimize the reconstruction error. But stacking multiple linear layers has the same effect as a single linear layer. Now the issue is that the authors only seem to specify that the decoder is linear. Perhaps they meant both encoder and decoder?\n",
      "If you want to learn basis and are just starting it's still a really good book. It provides a basic to intermediate level of understanding for most of the Deep Learning concepts. It's a great book to start learning\n",
      "Hello! Gilbert Strang Linear Algebra is absolutely good with the assistance of 3Blue1Brown Linear Algebra. (Also he has multivariate calculus, they're so good)\n",
      "I have the same question\n",
      "If computational issues are not a problem then just use AIXI, or else the bayesian posterior p(universe\\_where\\_x=True|our universe)  \n",
      "\n",
      "\n",
      "Since that's not really possible, then we don't really know. Companies like openAI are betting on scaling up deep learning, and it seems to be paying off. I personally think Bayesian deep learning should work, we just don't know how to scale it effectively yet. A common question that will really divide people is:  \n",
      "> How many big breakthroughs after deep learning will get us to human-level general AI, where a breakthrough is on the level of deep learning itself. \n",
      "\n",
      "* 0?\n",
      "* 1?\n",
      "* 2?\n",
      "* More?\n",
      "\n",
      "Personally, I think 1.\n",
      "From your questions you are posing, I think that you seem to understand the reason. The loss function assumes that the input is part of a [categorical distribution, which by definition enforces the vector to lie on the n-dimensional standard simplex](https://en.wikipedia.org/wiki/Categorical_distribution). This is why we can't just set every output to unity; the components of the vector must sum to one, which we achieve by applying a softargmax activation to the output. If we were to try to output all ones through the softargmax activation we will simply get a vector with each component equal to `1/n` (due to the normalization).\n",
      "Look around for pretrained object detection models to get started. “bird” is already a class that is labeled in some datasets. For example, check out the COCO dataset.\n",
      "Depending on the quality of the photos and your ability to preprocess, you might be able to use off-the-shelf models that are already built into deep learning frameworks like Tensorflow and PyTorch.\n",
      "\n",
      " https://link.medium.com/fys55wZgMcb\n",
      "I would go with the most elementary operations if you can (here reshape and reduce\\_mean). It helps knowing what these operations do. For instance, reshape is free because it only updates the tensor shape info and doesn't do anything to the data.  reduce\\_mean is as fast as it gets, especially along inner axis (because the data being summed is closer in memory).\n",
      "> * image rescaling ops (maybe?)\n",
      "\n",
      "I haven't looked at this for a while, [but the image rescaling in TensorFlow used to have major issues and would spit out the wrong results](https://hackernoon.com/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35). I would hope these are fixed by now, but I haven't tried them.\n",
      "\n",
      "> * convolution with a 3x1 kernel with weights (0.33, 0.33, 0.33) and stride 3\n",
      "\n",
      "> * reshape to 9x3x3 + reduce_mean on axis 2\n",
      "\n",
      "My gut tells me that the `reshape` + `reduce_mean` is going to be faster than the strided convolution, just because you are not having to initialize and perform a convolution. I would profile them however, because if the operation is performed on GPU it might be the opposite!\n",
      "\n",
      "I might suggest trying to one-shot encode each of the boards into sparse tensors of shape `(N,9,9,9)` (you choose which dimension is the channel dimension), as the distribution of each cell is not a continuous scalar but a discrete categorical vector. You can then enforce the one-of-each-category requirement by using `reduce_sum` on the rows/columns/3x3 blocks and comparing them to a vector of all ones. Also, Sudoku has the property that it actually doesn't matter what the categories are, so a fast implementation would treat all categories with the same operations to prevent training multiple copies of those operations.\n",
      "Elements of statistical leanring is a good book that is available as a free pdf. You should probably start with trying different basic techniques (as in tree versus nn versus svm) with a simple framework (sklearn) before getting into deep learning with TF + keras.\n",
      "Yes - here’s changing their movements to a new dance: https://youtu.be/PCBTZh41Ris\n",
      "My first thought was to look into capsule networks, but then I realized you meant changing pose of the object itself, as opposed to viewing the object from a different angle. Not sure about that...\n",
      "You could use [Neural Radiance Fields](https://arxiv.org/abs/2003.08934) to get a 3d model of your subject and something like [https://www.mixamo.com/#/](https://www.mixamo.com/#/) to do automatic rigging. Not sure how well this would work, I don't know much about 3d model rigging. A brief search turned up this [this](https://arxiv.org/abs/2005.00559) which looks helpful. Maybe follow the citation chain there\n",
      "\n",
      "There are a lot of papers around Neural Radiance Fields. One that may be particularly helpful to you if you wanted single image capabilities is [PixelNeRF](https://arxiv.org/abs/2012.02190).\n",
      "\n",
      "Finally, if you didn't want the full generalization of the above methods and you had images pairs of people in the \"T\" position and a free pose you might be able to use a CycleGAN architecture to move between the two.\n",
      "Just stumbled across this one; [Neural Body](https://arxiv.org/abs/2012.15838), it doesn't seem to be able to generate novel poses but could probably be made to do with modifications to the pose estimation network.\n",
      "So as I understand it, I don't think you would get K hyperparam combinations in K-fold CV since you are supposed to be training and validating the same hyperparams. This is done to average out the random effects of different datasets. So you're getting K models (since models are fit to different data), but the hyperparameters would be the same. \n",
      "\n",
      "However, there are methods like Grid Search or Random Search Cross validations which couple that k-fold cross validation, with hyperparameter search to provide cross-validated performance values for different models with different hyperparameter combinations (the chosen hyperparameter combination can then be fit to another held-out evaluation set to assess generalization error). I assume this is what you are referring to. \n",
      "\n",
      "To answer your question, for smaller hyperparam spaces, the distribution of hyperparam combinations might be small enough to identify with Grid Search or Random Search. for larger hyperparam spaces, I think you should look at optimization methods (like genetic algorithms, or simulated annealing) to more rigorously search through the hyperparam state space. I don't see the benefit of just taking the average.\n",
      "I would go data engineer if you can. Data analyst path will get you familiar with a specific type of business and how to ask the right questions but you may not get exposure to solving data problems at scale or building systems, which is a different way of thinking vs a super fast SQL data analyst. I can't speak first hand about entry level software engineer post but second hand you may get pigeonholed into the world of software 1.0, which is focused on building logic and fixing bugs, vs 2.0 which is solving for how to build quality data sets at scale to fuel the ML solutions\n",
      "I still have the [fast.ai](https://fast.ai) and the CS231 to go through, but I finished Andrew Ng's Deep Learning Specialization.\n",
      "\n",
      "Also, another thing that helped me SO MUCH was going through the Grokking Deep Learning book :[https://www.manning.com/books/grokking-deep-learning](https://www.manning.com/books/grokking-deep-learning)\n",
      "\n",
      "In this book, once in a while, the author asks you to memorise the previous algorithm - I followed the instructions, didn't take much, and now, once in a while, I write down a simple backprop algorithm on paper, from memory.\n",
      "\n",
      "The reason this helped me is because learning it line by line is hard and boring, so you get forced to learn the structure, and then derive the algorithm in your head. Thus, you will get a great intuition about how things work.\n",
      "Ask yourself, specially for easily interpretable algorithms like linear regression, is this variable something that could contribute if I look at the values manually, or is it not?\n",
      "\n",
      "My first impression with something like branch number (I guess some sort of ID for each branch) is that the algorithm could just \"memorize\" which branches have high sales and which not. So when you do inference, test with new data, if you give it a branch number that does well, it will immediately predict it will do well. I may be wrong, since actually it could be an important feature for other reasons that are escaping me. There's a lot of other factors that come into play.  \n",
      "\n",
      "\n",
      "Anyways, a good strategy for (applied) ML is to first develop using a simple model, look at how it performs, then iterate and improve. Thinking about how to perfect it from the first try will only result in wasted hours imo. I'm pretty sure even some of the most influential papers in this community were the result of many failed experiments where the researchers will never talk about all the things they tried and failed.  \n",
      "\n",
      "\n",
      "As for how to use the feature, basically you could input it as an integer value directly, let's say branch XXX, or convert it to a one-hot vector, a vector of 0s and 1s. If for example you have 4 branches, \\[1 0 0 0\\] represents branch 1, \\[0 1 0 0\\] branch 2 and so on. Another possibility is scaling it to a normalized version, for example substracting standard deviation and dividing by max value. The last is what they do with pixel values, that go from 0-255 usually, and are converted so they are usually on the range of -1 to 1. It all depends on your particular application.\n",
      "Unsupervised learning could mean that no human involvement is necessary. Labeling is done manually in supervised learning. For example, feed a model with a picture of a cat and the label \"cat\". Generally, language modeling is done in an unsupervised manner, as it involves assigning probabilities sentences to.\n",
      "In the last 5 years or so researchers have been calling such approaches: self-supervised learning. Like you said: It is different from traditional unsupervised methods, but it is also not human supervision which masks the tokens. There is also semi-supervised learning and transfer learning and reinforcement learning. All of these terms kind of have some overlap, and their boundaries become more porous, as new approaches mix-and-match from the different approaches, and move further and further away from strict, and clearly-defined, methods, where 15 year ago vast majority was supervised learning, and the rest was called unsupervised.\n",
      "As a rule of thumb your input dimensions while testing should match the input dimensions while training to get positive results.\n",
      "Why don't you buy a $1000 arm, a $300 camera and try to *do* something? If that's what you *really* want? \n",
      "\n",
      "Going to university helps, but you need to take control in the end.\n",
      "\n",
      "I think robotics is a lot of mechanical and electrical engineering. The actual control aspect of it is something that just requires one to throw some money at these days. There is no computer science left there at this point for almost all applications. \n",
      "\n",
      "If you want a full blown AI, forget about it. It's possible already in theory, but the computers required to do that do *not* exist and will not until you retire.\n",
      "I'd suggest you to look up what research is happening at some top robotics labs. At CMU, MIT, Stanford, Caltech etc. From there on you will have some idea what robotics research is all about. From my knowledge robotics research consists of much more than reinforcement learning.\n",
      "If interested in reinforcement learning and robotics look at the work of Pieter Abbeel. Look at his slides/course notes for introductory courses. Look at thesis subjects his students are writing. Look at what companies spin-off from lab research. \n",
      "\n",
      "* [https://www.youtube.com/watch?v=xWPViQ6LI-Q](https://www.youtube.com/watch?v=xWPViQ6LI-Q)\n",
      "* [https://people.eecs.berkeley.edu/\\~pabbeel/publications.html](https://people.eecs.berkeley.edu/~pabbeel/publications.html) \n",
      "* [https://covariant.ai/](https://covariant.ai/)\n",
      "\n",
      "But also think: which companies right now really need extremely accurate and broad face recognition and detection? Probably a hand-full. Then look at the quality of the talent they are hiring and poaching from the small pool of elite computer vision researchers. Instead of going for fitness & health coach, you would be going for NBA-player psychologist (and maybe re-school for fitness coach if that world-star career path does not work out).\n",
      "\n",
      "For robotics, manufacturing is the oldest and most established one. Every country has at least 1-2 companies which import from Asia/locally source robot arms, belts, panels, etc. to automate a manufacturing process, design a factory line. You'll be sitting with a laptop next to a giant arm, to test out a new path algorithm, or use OR to solve for a most efficient floor layout, or use computer vision to discard faulty objects, or show Asian R&D execs how you made their arm do things they did not imagine to be possible.\n",
      "If you’re using dropout, you can continue to use dropout during testing and samples which have the greatest variation in classification can be called outliers. Then you only have to go through those rather than the whole dataset. I can’t remember which paper did that, but a quick google scholar search will probably turn it up.\n",
      "It's all more or less the same: early stopping, schedulers, etc. There is some theory for both, but in practice, it amounts to just finding some working heuristic. You could try cyclic cosine annealing combined with early stopping. CCA decays the learning rate rather aggressively in a schedule, so you could check the loss at each minimum of the scheduler and stop at the best one. There's just not a solid way to predict how to control the learning rate if you only use first-order optimization (which we most all do)\n",
      "Do you have metrics monitoring set up (like Tensorboard)? You could test both few times and see how loss score or other metrics behave.\n",
      "Transformer encoder is bidirectional - it looks at the whole sentence at once. \n",
      "\n",
      "Transformer decoder is unidirectional - it can only look at past words. We explicitly mask the connections so it cannot look at 'future words'. This might seem a bit strange, but it's the basis of all autoregressive models. Autoregressive models have always been used for tasks such as text generation and translation.\n",
      "\n",
      "BERT argues that for common language tasks such as sentiment prediction, you don't really need unidirectional models, like GPT is. It's intuitive you would get a better performance by looking at the whole sentence at once.\n",
      "\n",
      "GPT is mostly used for its generation property - it can generate stories, fake news, etc. Turns out you can do a ton of zero-shot tasks with it. You can also evaluate probability of a sentence with it which is nice. Also, it seems OpenAI pretty much thinks this framework is the key to AGI (they now use GPT on images, audio, ...) \n",
      "\n",
      "In the original Transformers network, they looked at original sentence with encoder network - since you should be able to look at the whole original sentence, but then decoded it with decoder network - which is autoregressive.\n",
      "I follow the AIDL group on facebook.\n",
      "[This](https://www.cin.ufpe.br/~tfl2/artificial-intelligence-modern-approach.9780131038059.25368.pdf) for AI more generally. [This](https://mml-book.github.io/) for ML maths. [This](https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf) to see ML principles in a more applied setting.\n",
      "> On StyleGAN face images are aligned, I assume this gives the network result some kind of boost?\n",
      "\n",
      "The alignment *dramatically* simplifies the problem for any generative model, as it means it can fit to a well defined/constrained output distribution instead of one which is far more complex.\n",
      "\n",
      "> How should I proceed if I have a dataset with non human faces, such as trees? How should they be aligned?\n",
      "\n",
      "I'm not an expert, but I would say align and scale them as well as you can.\n",
      "BTW, you can use code blocks with four spaces:\n",
      "\n",
      "    if 1 * 2 < 3:\n",
      "        print \"hello, world!\"\n",
      "\n",
      "> i dont know how to add a neural network for this code\n",
      "\n",
      "I am quite confused by this. Your code already is instantiating a model! You have a Sequential container with 2 dense layers, the last one having a softmax function as it's activation. \n",
      "\n",
      "    model = tf.keras.Sequential()\n",
      "    model.add(tf.keras.layers.Dense(10,activation='sigmoid', input_shape=(784,)))\n",
      "    model.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
      "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics = ['accuracy'])\n",
      "    model.fit(x_train, y_train, epochs=10, verbose=0)\n",
      "That code is a neural net. Two layers with 10 neurons each.\n",
      "What are you trying to accomplish? On paper it should be no contest, but none of these are modern solutions. I would be concerned with driver and feature support for these aging products. If these are the only products available to you, then I think the choice is clear. You need to remember that the Tesla doesn't have a display output, so if you need one you'll have to find another solution for that.\n",
      "> The weighted network has been created and stored in .h5 file\n",
      "\n",
      "I first would direct you to the [Serialization and Saving tutorial](https://keras.io/guides/serialization_and_saving/) if you haven't read it yet. My initial reaction was to tell you that the `.h5` file only contained the weights, but I haven't used Keras for a few years now and double checked. Brushing up on the documentation, this is not the case; just make sure that you are saving the model using `keras.Model.save()` instead of `keras.Model.save_weights()` and you should be good.\n",
      "\n",
      "> **Now I would like to create an application (preferably using JS)** which takes an input image and I give the output as the predicted result according to my model. Could you suggest any ways on how to do this?\n",
      "\n",
      "Clarification on the strong text: are you preferably trying to create a *web application* or a *desktop application using web technologies*? I can only assume that the solution will vary dramatically based on this.\n",
      "> Is ML bad for my laptop, it's using Ryzen 7 4700U and Radeon Graphics, my laptop has a really thin bodies and **doesn't stand against heat for a long time**.\n",
      "\n",
      "Heat shouldn't be much of a problem unless it is throttling; I would suggest looking into undervolting if you are hitting either the package power limit or package thermal limit and can deal with debugging the potential instability issues that may arise when tuning it (BSODs, hard halting/shutdowns under load). (If you are up to the task and interested, I suggest [ThrottleStop](https://www.techpowerup.com/download/techpowerup-throttlestop/) for doing this.)\n",
      "\n",
      "> Do i had to run my ML model locally, or should i not ?\n",
      "\n",
      "You absolutely can run models locally, but what is realistic to run locally depends on what kind of models your interested in. More traditional models and small neural networks can be fit and sampled without GPU acceleration on a modern laptop no sweat, and GPU acceleration could make medium-sized neural networks in the range of plausibility. I would probably say that decently large neural networks are out of scope, but maybe I am just really impatient. (The last time I tried to train a neural network on a laptop I was using an Ivy Bridge i3!)\n",
      "\n",
      "> For cloud alternatives, i have used Google Colab and Kaggle kernel, are they good enough for you guys to do your thing, or is it too slow for real ML engineer ? What cloud services do you use for training ML ? (Especially the free one, for learning and competitions)\n",
      "\n",
      "If you are not trying to train a large network for production and just want to play around and learn, Colab and Kaggle are exactly what you are looking for. No, your not going to get great performance, but Colab GPU instances are a night and day difference to local training on a laptop. Follow Google's guidance of not using a GPU instance unless you need it, as they will limit your access if your are using it too much. (i.e. Figure out your dataset creation and preprocessing, as well as model architecture on a CPU instance to make sure it works, and then switch to a GPU instance for training. Manually terminating your instance when you are done also helps in this regard.) They are pretty lenient, but they are providing the service for free after all!\n",
      "> My question is, can anyone please suggest a comprehensive and updated guide to set up a Deep Learning environment on Windows 10?\n",
      "\n",
      "WSL not too long ago gained the ability to interface with the GPU for compute. I think the current recommendation is to just setup your favorite distro in WSL and work with it like any other Linux machine.\n",
      "The statistical learning lectures (first link) I believe are a little less superficial than the other one. But they cover different things. The statistical learning lectures will not teach you about explanatory analysis (inference, hypothesis testing, etc), only about prediction.\n",
      "Treat it as a language and try to predict the next token. So any off-the-shelf architecture that can predict the next token from a text corpus should be a reasonable starting point, e.g. RNNs or Transformers (GPT).\n",
      "> This model seems like it would work(although I'm really just a beginner at machine learning so correct me if it doesn't make sense) but the main problem is the lack of data.\n",
      "\n",
      "I congratulate you on coming to the realization that you might not have enough data. Quality of data can make or break your attempt to create a generalizable model.\n",
      "\n",
      "> tl;dr: I need to train the model above with a minimal amount of data, what are ways to do so?\n",
      "\n",
      "*You can try to augment your dataset.*  You can introduce noise into the data, or if we have more knowledge of the system we have some other options available.\n",
      "\n",
      "For the sake of demonstration, I'll imagine your input vector to be five-dimensional and normalized to range from `0.0` to `1.0`, with each component the flex of each finger on your hand. This overlooks useful data in this task (a high-five commonly has the fingers spread for instance while a thumbs-up has them against each other), but it helps in distilling the concept down.\n",
      "\n",
      "Suppose that the ideal input vectors are `[0.0, 0.0, 0.0, 0.0, 0.0]` for a high-five and `[0.0, 1.0, 1.0, 1.0, 1.0]` for a thumbs-up.\n",
      "\n",
      "We could add a small amount of noise to the system to augment our dataset and get something like `[0.07, 0.98, 0.93, 0.94, 0.96]` to fill the gap.\n",
      "\n",
      "Another solution (if we can assume that the regions of each class are convex) is to take a linear combination of the training samples. For example, if we had three training vectors `[0.05, 0.91, 0.91, 0.95, 0.91]`, `[0.05, 0.95, 0.97, 0.96, 0.93]`, and `[0.00, 0.95, 0.93, 0.97, 0.93]`, we could for instance sample a random weight vector from the three-dimensional standard simplex `[0.17, 0.69, 0.13]` to produce a new vector that doesn't exist in the original dataset `[0.04 , 0.93, 0.94, 0.95, 0.92]` that is guaranteed to lie within the convex volume bounded by the data set.\n",
      "\n",
      "When working with images, augmentation often involves affine transformations and adding per pixel noise. This is incredibly useful for training classifiers and GANs!\n",
      "\n",
      "> Right now, my model just looks like this\n",
      "\n",
      "> `12(Input) -> 7(Dense) -> 3(Dense) -> 2(Dense)`\n",
      "\n",
      "Make sure you have a softargmax activation on the output of your last layer! You could use a single output node and binary crossentropy of course, but you have already indicated that you would like to extend this to more classes.\n",
      "\n",
      "I suggest looking into some more \"traditional\" classifiers, like nearest-neighbor and Support Vector Machines. They may be a better fit for your task if you don't need a classification probability at the output!\n",
      "Probably not. If you're a beginner to ML and want to learn ML, use Python.\n",
      "As a fellow Rustacean, Rust isn't even worth learning for tooling. It's literally all Python. I've tried pushing to replace our Java and C++ stack for Rust at work, but the language is just too hard and young to transition into.\n",
      "[Grant Sanderson has you covered over at 3Blue1Brown.](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
      "It makes complete sense to convert 2D convo networks meant for images to 3D as you suggest, if the sequence of images are slices of volumetric data.  The only problem is that it takes a lot more memory and time in 3D.\n",
      "\n",
      "I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \n",
      "render large Jupyter Notebooks, so just in case, here is an \n",
      "[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n",
      "\n",
      "https://nbviewer.jupyter.org/url/github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_style_gan.ipynb\n",
      "\n",
      "Want to run the code yourself? Here is a [binder](https://mybinder.org/) \n",
      "link to start your own Jupyter server and try it out!\n",
      "\n",
      "https://mybinder.org/v2/gh/jeffheaton/t81_558_deep_learning/master?filepath=t81_558_class_07_3_style_gan.ipynb\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "^(I am a bot.) \n",
      "[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n",
      "[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n",
      "[^(Author)](https://johnpaton.net/)\n",
      "I can take a closer look tomorrow morning (local time), but I have a hunch that you are using a CPU instance rather than a GPU instance. Try switching to a GPU instance if you are not using one, as **StyleGAN *requires* CUDA** (and hence a NVIDIA GPU).\n",
      "Depends on the network. What you need to understand is that the number of parameters doesn't determine the topology of said network. It can, but normalization layers, activation functions and their ordering have a greater effect on it.\n",
      "\n",
      "Think of a trivial example of stacked linear layers without nonlinearity between them. You can have any number of parameters but they will still have the topology of a single linear layer because they have the same power as one.\n",
      "nn.Embedding is just a tool - the vqvae codebook is just one way to use this tool.\n",
      "\n",
      "So you'd implement the codebook with NN.embedding, like here https://github.com/ritheshkumar95/pytorch-vqvae/blob/master/modules.py#L70\n",
      "[deleted]\n",
      "\n",
      "I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \n",
      "render large Jupyter Notebooks, so just in case, here is an \n",
      "[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n",
      "\n",
      "https://nbviewer.jupyter.org/url/github.com/Puzer/stylegan-encoder/blob/master/Play_with_latent_directions.ipynb\n",
      "\n",
      "Want to run the code yourself? Here is a [binder](https://mybinder.org/) \n",
      "link to start your own Jupyter server and try it out!\n",
      "\n",
      "https://mybinder.org/v2/gh/Puzer/stylegan-encoder/master?filepath=Play_with_latent_directions.ipynb\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "^(I am a bot.) \n",
      "[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n",
      "[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n",
      "[^(Author)](https://johnpaton.net/)\n",
      "It depends on the problem. Looks like this is a network that has a warmup. A more popular network with warmup is BERT, and it is a necessary step because BERT is a load of unstable garbage that diverges without it.\n",
      "If you're not getting any luck here, you could try searching linkedin for Data Scientist and messaging a bunch of them until one responds.\n",
      "u/Serious-rams786 is possible to explain in more detail about the logical part of the code you are struggling with?  \n",
      "\n",
      "\n",
      "Using python for machine learning:  \n",
      "You should check out [fastAI](https://course.fast.ai/) which have a good course about learning ML with basic knowledge of python.\n",
      "This might be way off the mark, but here is my understanding.\n",
      "\n",
      "Taking the kernel between two datapoints, k(x,y) is equivalent to taking the inner product in some feature space. Let the map which moves from input space to feature space be Tx. So if x is your input, Tx is in some abstract feature space. Then, we have k(x,y) = <Tx, Ty>, where <., .> denotes the inner product between two elements in this feature space.  When you have any inner product, you can use it to define a norm, so <Tx, Tx> = ||Tx||^2 , where ||.|| is a norm (or distance) in this feature space.\n",
      "\n",
      "When you do KNN, you measure using this squared distance, ||Tx-Ty||^2 , which if we put that in inner product terms, is <Tx-Ty, Tx-Ty>. Based on the linearity of the inner product, we can expand this as <Tx-Ty, Tx-Ty> = <Tx, Tx> - 2<Tx,Ty> + <Ty,Ty>. And in terms of kernels this is k(x,x) - 2k(x,y) + k(y,y). So to do KNN with kernels you just use sqrt(k(x,x) - 2k(x,y) + k(y,y)) as your distance.\n",
      "\n",
      "\n",
      "Normal KNN based on the euclidean distance is just taking the kernel to be the standard euclidean dot product.\n",
      "\n",
      "You can look at this idea more in https://www.researchgate.net/publication/220578072_Kernel_Nearest_Neighbor_Algorithm .\n",
      "Kernel trick is usually applied to SVMs. I’ve never heard of kernel transformations with nearest neighbors. A quick google search reveals that there is some research in this topic. Usually to get the most out of KNN you should scale the data. Why would you like to apply kernel transformations?\n",
      "Loss is unrelated to LR. Without a lot of assumptions about a problem you can't even see the correlation of loss and LR. There might be a trend that as LR goes to 0, the loss fluctuates less because the weights fluctuate less, but I can give you a stochastic loss function that is partly determined by RNG and you're right back at the start.\n",
      "I like Bertseka's book. Also, since you linked a course on Convex Optimization, I should mention that Stephen Boyd's book and lectures are amazing for that topic.\n",
      "Yes.\n",
      "\n",
      "Calculus, Linear Algebra, Probability... the list goes on. ML is pretty much using math to model systems that we cannot otherwise model in a conventional manner.\n",
      "\n",
      "Understanding the basic underlying math is critical: You should be able to understand the concepts of gradients, matrix multiplication and other products of multidimensional arrays. Knowing how to perform them by hand isn't that important though. (That's why we use computers!)\n",
      "Yes, Learning maths is important so you have an idea what your model is doing.  \n",
      "I recommend reading this article: [https://www.ycombinator.com/library/51-learning-math-for-machine-learning](https://www.ycombinator.com/library/51-learning-math-for-machine-learning)  \n",
      "\n",
      "\n",
      "Gives a decent overview of what topics you need to learn. And provides resources on how to learn the topics.\n",
      "Study linear regression and logistic regression, it will give you a taste of linear models and their variants\n",
      "\n",
      "Next study optimization, overfitting, bias-variance trade off, and loss function\n",
      "\n",
      "Get to know neural networks, this is the vocab you will need: deep vs. shallow, epoch vs. batch size vs. iteration, weights, weight initialization, softmax, pruning, and activation functions\n",
      "\n",
      "Study backpropagation and gradient descent\n",
      "\n",
      "Finally, study the newer topics from the last 5 years. dropout, batch normalization, transformers, encoder/decoder, categorical embeddings, adversarial networks, Adam, skip connections, semisupervised pre-training, those are some of the big ideas.\n",
      "\n",
      "If you had to study two or three architectures I’d say AlexNet, VGG, and ResNet are a good place to start\n",
      "There's a ton of connections between topics in machine learning and so it's not really a question of what's important to a particular area as much as it's \"how deep do you want to go?\" There are a few things that you won't be able to avoid needing if you're working on anything nontrivial, though. \n",
      "\n",
      "- probability- know the basic axioms of probability, how marginal/conditional/joint distributions relate, the algebra of random variables (what happens to e.g. expectations, variance, covariance, etc. when you do arithmetic with random variables?), what compound and mixture distributions are, log likelihood functions and why they're frequently optimized, stuff like that.\n",
      "- Some topics from information theory, closely related to the above. The concept of Shannon entropy, its interpretation in terms of code lengths, conditional/joint entropy and the algebra of those, relative entropy (KL divergence) and mutual information especially in terms of the code length interpretation.\n",
      "- multivariate calculus- be comfortable with integration, partial derivatives and gradients, the method of Lagrange multipliers for constrained optimization, convexity and its implications in optimization, and the basics of simple optimization methods like Newton's method (and multivariate extensions), gradient descent, line search.\n",
      "- linear algebra and matrix calculus- know what the rank, kernel, and span of a matrix are, what positive definite matrices are and why they're important, matrix inverses and pseudoinverses, eigendecompositions/SVD, and be comfortable with matrix algebra and finding gradients of functions of matrices.\n",
      "\n",
      "Can you build a neural network to solve some or other problem without knowing any of this? Absolutely. You could get quite far in terms of practical application on many problems. You won't, however, be able to understand the mechanics of what you're doing, you'll be fiddling with knobs and buttons until things work. There's a lot to be said for jumping in with practical problems that you can use to motivate your learning- you absolutely should do that. Imo that's the most important. All I'm saying is that this should be something you're working on at the same time. These are the basic language of machine learning in most areas- if you want to understand the higher level ideas in a topic, you have to know your vocabulary. This is, in my opinion, the foundation you need to build on top of. Without it, you won't know how things connect, why things work the way they do, and most importantly why they *don't* work when they break. You can drive a car without knowing anything about how an engine works but only until you run out of oil, break a timing belt, lose a spark plug, etc. But there's no AAA or mechanics in machine learning. You can and should get help from resources like this subreddit, professors when possible, textbooks, other people's code, etc. But that doesn't solve your problem for you. It's frequently more like a car manual for an older model than yours, and some things are in different places and others are just different. You'll still need to do the fix yourself, and adapt what you learn to the distinct problem you have.\n",
      "yes, of course. forward pass requires a huge amount of computations in many cases (e.g. large input images)\n",
      "You could use a one-hot encoding so that all of your data is inequalities. For example one is <20 one is 20-25, the next is 25-30 and so on.\n",
      "Yeah the new stuff is probably complicated enough you won't be able to implement it.\n",
      "\n",
      "If you want a fun little exercise, you could always try implementing LSTMs or GRUs, even though even with that it's going to be a workout to get it working smoothly.\n",
      "\n",
      "You could try implementing Batch Normalization but that is too easy to implement for anything called a project and you can't even explain what it does (because no one understands truly what happens). Same thing with transformers.\n",
      "\n",
      "You can try to build resnets but it's unlikely you'll have the computational resources to train them.\n",
      "\n",
      "So overall, what you can engineer and understand is stuff from the last century, more or less, definitely not the new stuff which lately delves deeply into topology and graphs.\n",
      "So, I'm guessing that by \"noise\" you mean some combination of \"mods make mistakes\" and \"some bannable offenses are unobserved by mods\", but it's not super clear. Is that correct? \n",
      "\n",
      "There's also the noise in the text data itself, in the sense that a majority of words from any user, banned or not, will be irrelevant to the prediction. That's another hurdle, but there are lots of tools at your disposal there.\n",
      "\n",
      "You say you're interested in classifying *when* a user will be banned. Can you be more precise? For example, do you want to predict whether a particular message (or some collection of messages like recent history) leads to a user being banned? That would be looking at causation, and you'd need some data from mods in the form of \"I banned this user because of these particular messages\". Are you interested in predicting whether a particular message came from a user who was banned at some point in the future or not? That would mean you'd need labels of banned users only.\n",
      "\n",
      "These might sound pedantic but the paths you can take for an approach might be very different based on the answers!\n",
      "\n",
      "Some topics to look into in the meantime- PU learning (positive unlabeled, where you have labels for one class- banned users, for example- and the rest are unlabeled), multiple instance learning (where you have labelled collections of samples only- a \"bag\" of samples will have a negative label if none of the samples are positive, or a positive label if any of the samples are positive. Here that could be messages by a user- you know that a user who was banned had at least one bannable message, but you don't know which one(s), and for users who weren't banned you know that all of them are negative).\n",
      "This is a computer vision problem, you need to have some familiarity with matrix operations and image processing, and don't think it will be easy but below I explain the process.\n",
      "\n",
      "First you need to align the image: [https://www.pyimagesearch.com/2020/08/31/image-alignment-and-registration-with-opencv/](https://www.pyimagesearch.com/2020/08/31/image-alignment-and-registration-with-opencv/)\n",
      "\n",
      "then you need to compute the canny edges of both images:\n",
      "\n",
      "[https://opencv-python-tutroals.readthedocs.io/en/latest/py\\_tutorials/py\\_imgproc/py\\_canny/py\\_canny.html](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html)\n",
      "\n",
      "now you need to match the images with 2 loops, one for width and one for height, you start to shift one image over another and compute the difference between the canny of both images using a metric like MSE.\n",
      "\n",
      "With the smaller distance metric you have the best matching for both images, so in this position you compute regions where the cannys are different given some threshold, so you get the solution.\n",
      "The sigmoid activations wil work for MNIST(binary images) but they'll saturate for CIFAR-10.  Try changing them to 'relu' activations.  Also a fully-connected network will only do so well on anything beyond MNIST, maybe try CNNs?\n",
      "I do have to promote the ML1 course from the University of Amsterdam: https://youtube.com/playlist?list=PL8FnQMH2k7jzhtVYbKmvrMyXDYMmgjj_n\n",
      "\n",
      "Quite honestly, I found it much better than any other ML course online. Beware though that it is a bit more heavy on maths than the more popular courses. However, this will only be to your advantage in the end\n",
      "\n",
      "EDIT: They‘re using Bishop as textbook\n",
      "There are authorship analysis approaches in NLP. You could have a look there\n",
      "\n",
      "I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \n",
      "render large Jupyter Notebooks, so just in case, here is an \n",
      "[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:\n",
      "\n",
      "https://nbviewer.jupyter.org/url/github.com/Puzer/stylegan-encoder/blob/master/Play_with_latent_directions.ipynb\n",
      "\n",
      "Want to run the code yourself? Here is a [binder](https://mybinder.org/) \n",
      "link to start your own Jupyter server and try it out!\n",
      "\n",
      "https://mybinder.org/v2/gh/Puzer/stylegan-encoder/master?filepath=Play_with_latent_directions.ipynb\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "^(I am a bot.) \n",
      "[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n",
      "[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n",
      "[^(Author)](https://johnpaton.net/)\n",
      "> I have read that f'{} requires python3\n",
      "\n",
      "You could get around it by replacing the f-string... but why are you trying to run it under Python 2? That's your problem. Use Python 3.\n",
      "\n",
      "> but I cannot run python3 without tensorflow 2.x\n",
      "\n",
      "Use the `%tensorflow_version 1.x` cell magic as the first line of the first cell that you run.\n",
      ">And getting this error:\n",
      "File \"/content/stylegan-encoder/encode_images.py\", line 73\n",
      "img.save(os.path.join(args.generated_images_dir, f'{img_name}.png'), 'PNG')\n",
      "\n",
      "What's the actual error here? This is just where it occurred, not what it is.\n",
      "\n",
      "This sounds like a problem of dependency management, though. [The preprocessing module](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) for example exists in the current version of tf.\n",
      "\n",
      "Regardless, there's some good documentation about [migrating tf 1 code to tf 2](https://www.tensorflow.org/guide/migrate) that gives a simple but low-tech fix:\n",
      "\n",
      "```\n",
      "import tensorflow.compat.v1 as tf\n",
      "tf.disable_v2_behavior()\n",
      "```\n",
      "\n",
      "Which should allow you to run code as is (unless it depends on the contrib modules) but without the benefits of tf 2. The rest of that page is worth a read. You really should be using python 3 and tf 2 for a number of reasons but with this it shouldn't be too much of a problem.\n",
      "Getting 85% validation accuracy on a binary classification problem isn't phenomenal but it does mean that your model is definitely learning *something*. So that's good news. The unstable performance per epoch can be a symptom of a learning rate that is too great- stepping too far in the direction of the gradient, then overcorrecting in the next epoch, etc. I would try lowering the learning rate.\n",
      "\n",
      "It's also possible that you're not training long enough, as odd as that may seem. It's possible for a model to look like it's overfitting (or more likely, reaching a plateau with validation but improving on training) for a while and then move into a \"regime\" where the generalization ability of the model starts to improve again. Try dropping the learning rate and running it for twice the number of epochs, see what happens.\n",
      "\n",
      "It's also entirely possible that this is just the best you can do on this dataset. Do you have access to larger images? 100px square is not all that large, relatively speaking, and if you're scaling the images down you may be removing important information. Same goes for color. It may be the case that working with larger images is too much of a burden computationally with the full dataset, but I would be surprised if you needed the full 13000 image dataset for a binary classification problem. You could try out larger images with less than half the dataset.\n",
      "\n",
      "My first step personally would be to ensure that what I'm seeing isn't an artifact of the train/validation split. Try cross validation with 5 or more folds (the more the better, but also the more computational burden). If you're seeing different behavior than you did with the fixed splits, then that's something you need to isolate before you can address any issues with the model directly.\n",
      "I imagine that part numbers are standardized so what you would actually need is not AI but a simple lookup from part number to a picture of the part and then a search procedure for that picture of the part in the book. Or am I misunderstanding what you want to do?\n",
      "One of my projects a while back was [spotting trees with satellite imagery](https://www.tobiolabode.com/blog/2020/7/12/tree-machine-learning-project).\n",
      "\n",
      "The results have a lot to be desired though. \n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Resources that talk about the topic:\n",
      "\n",
      "[https://medium.com/omdena/how-to-build-a-high-impact-deep-learning-model-for-tree-identification-e25c5655ae16](https://medium.com/omdena/how-to-build-a-high-impact-deep-learning-model-for-tree-identification-e25c5655ae16) (nice blog post of a classification project)\n",
      "\n",
      "[https://towardsdatascience.com/deep-learning-with-satellite-data-b78b20708de](https://towardsdatascience.com/deep-learning-with-satellite-data-b78b20708de) (short blog explaining a classifaction project)\n",
      "\n",
      "[https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/)\n",
      "\n",
      "[https://earthengine.google.com/](https://earthengine.google.com/) Sign up for an account to get satellite imagery data\n",
      "It's hard to give you directions without knowning more of your data and how familiar you are with ML. Also, what do you mean by code release? If it's a classic software release, I'd suggest a very basic approach. You need to create the following features looking at past releases:\n",
      "\n",
      "\\- number of new lines of code\n",
      "\n",
      "\\- number of bugs that have been solved\n",
      "\n",
      "\\- number of bugs that took significantly long time to be solved (you would have to use a kind of comparison metric here)\n",
      "\n",
      "\\- number of non-regression tests that have been performed\n",
      "\n",
      "Then you create the label associated with each release: successful or not (0 or 1).\n",
      "\n",
      "Also bare in mind that for the model to be accurate enough you would need a good number of data; not sure how many past releases you can have access to.\n",
      "I think using unsupervised learning to categorize households and climate type is a good idea, but I actually don't know if you need different energy models. \n",
      "\n",
      "All buildings experience the same physical processes for heating/cooling/lighting and utility use, which in turn is a function of: weather, occupant usage, and the physical construction of the building. If you can successfully generalize your model around those more foundational ideas, you may be able to get away with one model. \n",
      "\n",
      "For example, heat transfer from ambient air is a linear function of temperature difference, thermal resistance of building envelope (which you can estimate based on climate zone) and envelope area. So if you can break down your supervised learning feature set to capture some of the physical properties of the houses, and number of occupants (which is linearly relates to electricity use), you can probably use one energy model. I once managed to capture annual energy consumption for a series of houses in different climate zones with just a linear regression on some building envelope data - so I know this is possible. \n",
      "\n",
      "I would look at the literature on building energy simulation, which contains a lot of domain knowledge that you can incorporate into your model. For example, when you're adding weather data to the featureset, make sure to include the maximum and minimum drybulb temperatures. That'll approximate the \"peak load\" condition which is how mechanical engineers determine HVAC sizing, which in turn will inform the efficiency of the heating/cooling/ventilation systems. \n",
      "\n",
      "Also, have you considered a time-series prediction model? I don't have a lot of experience with time-series predictions but the two things that spring to mind is a time-series prediction with ARIMA or GARCH forecasting methods. The advantage of these methods is that you can more explicitly capture the time-dependent (in certain cases, markov) nature of weather and energy, versus supervised learning.\n",
      "Yes definitely doable. Idk that a transformer is needed but might work. Simple NN might work. RL could play the game.\n",
      "Every column will have a different scaling factor for exactly the reason you described. In the case of images, each image channel (R/G/B) typically gets a different constant too (but these scaling factors are shared over the image pixel dimensions).\n",
      "I had a similar situation as you 3 years ago when I graduated from aerospace engineering. I went and got a Master’s in ML and I at least feel confidant I could get a job in ML, but I’m working on a PhD instead. There are a few programs out there that are tailored for engineers trying to transition into more computational areas\n",
      "Still looking for this\n",
      "I direct you to [MarI/O](https://www.youtube.com/watch?v=qv6UVOQ0F44), and more importantly, [the follow up](https://www.youtube.com/watch?v=iakFfOmanJU).\n",
      "We can't compute the marginalization over z in closed form so we approximate it by doing monte-carlo samples from p(z).\n",
      "\n",
      "If you're trying to compute integral{p(z) f(z)}dz, for some function f, then you are marginalizing over z. This integral is relatively high dimension so we can't compute it. We  can rewrite the integral as E(f(z)), where E is the expectation operator, with samples of z drawn from p(z). Looking at the formula for the expectation operator, this is exactly the same as the integral we want to compute. We then approximate the expectation by drawing a K samples from p(z) and averaging over the values of f(z).\n",
      "Assuming the training set that the \"original\" DT is trained on is also not increasing to infinity, I think you're right. As the DT-generated training set increases to infinity, the DT will produce training data where joint probability distribution exactly reflects the mutual information (the reduction in entropy in outcome by observing some feature) that is used to generate the split. \n",
      "\n",
      "That joint probability distribution is then used to generate a new tree, based on the information gain of each branch split, which of course is equivalent to mutual information at larger sizes. So you're just going to end up re-creating the same tree.\n",
      "I feel like this makes sense if you reframe your problem as a descriptive analysis, rather then predictive analysis. I have seen evidence this can be done for regression\\[1\\], and I don't see why it wouldn't be the same for DTs. In linear regression, the descriptive interpretation of regression (which is typically used for prediction) is that the regression coefficients give you the conditional distribution of y given some feature x\\_n. So the slope of the regression line describes the expected importance of each feature to y.   \n",
      "\n",
      "Your problem is therefore a descriptive analysis problem, in which the information gain in each decision tree feature split tells you the importance of each feature. There's no forecasting involved, so I don't think you need the test set. Of course, we have to keep in mind the sample data is not representative of a population, and therefore contains random noise. \n",
      "\n",
      "\\[1\\] [https://statmodeling.stat.columbia.edu/2017/03/07/descriptive-analysis-using-regression-models/](https://statmodeling.stat.columbia.edu/2017/03/07/descriptive-analysis-using-regression-models/)\n",
      "- Could measure agreement of unsupervised clusters with contract type as proxy for accuracy.\n",
      "- But lets take contract type id as cluster id.\n",
      "- Blackbox + SHAP/LIME usually too complex/overkill for cluster analysis. https://github.com/scikit-learn-contrib/skope-rules/blob/master/notebooks/demo_clustering.ipynb gives good results with very interpretable, diverse, decision rules.\n",
      "- Without cross-validation or validation set, you indeed are leaking information. May be good enough for your purpose, but probably not for scientific method. Blackbox models increase risk of such overfit. SHAP+LIME may start to memorize/act weird when explaining train samples (and are less interpretable, because these vary per sample). The EBM uses holdout and bagging to give unbiased predictions and explanations for the train set: https://github.com/interpretml/interpret/blob/develop/benchmarks/EBM%20Classification%20Comparison.ipynb\n",
      "In my view, one-hot vectors can absolutely be considered as a sparse data type. Btw, *sklearn* library (a common package in DS) lets us precise if we want the data in optimized form ```OneHotEncoder(sparse=True)```\n",
      "Could it be due to the fact that these one hot vectors are placed into a tensor that is eventually passed into a ML model which requires numerical values not Nan values. I assume most ML frameworks do not default to read sparse data structures.\n",
      "Docker might be a good tool to look into. You can create a base image and you’re students will be able to pull it from the web and mess with it however they want\n",
      "Try to take another ML class if you can. Sometimes the material sticks better the second time.\n",
      "\n",
      "ML is about pattern recognition. Given a set of data, you might want to classify a tumor as benign or malignant based on a set of diagnostic factors. This is equivalent to estimating a function or decision rule conditioned on the available data. That’s what you’re doing.\n",
      "\n",
      "If we make some light assumptions then the model should be useful. For instance, if new data comes along with the same distribution as the sample data, then the model’s performance on the new data should be related to the model’s performance on the training data.\n",
      "\n",
      "We usually validate our models with cross-validation.\n",
      "ML can be used as a black box but when applying to the professional world, you need to understand what you are doing simply because you will have to convince users that this is the right method to use.\n",
      "\n",
      "Now to better answer your questions, KNN is used to predict a class for a new sample. The accuracy tells you whether the model is successfully finding patterns. If you have a poor accuracy, it might be better to change the model or (in most cases) review the features. There are numerous accuracy metrics (including F1 score) and they should be used depending on your problem. For example, some metrics are better suited for imbalanced datasets (Precision-Recall curve might be better to use instead of the ROC).\n",
      "\n",
      "Regarding validation, yes you would validate your model on unseen datasets. If not, a good accuracy will simply tells you that your model fits well the data but you don't know whether it is relevant when used with other data.\n",
      "Machine Learning should best be tied to a clear problem, and take into context its place within a wider pipeline of decision making.\n",
      "\n",
      "What could you do with a diabetes-detection-from-shopping-cart-selection? Could be moderately useful for policy and government statistics bureaus. Has some academic social science value to show you can (and so can Walmart) predict disease from shopping behavior. Can estimate causal effects of eating in public restaurants and spread of COVID. Marketing and sales can work on future deals of certain products, if these correlate with diabetes, and it is known that diabetes is on the rise (and so would demand, and great demand for early-made good deal is profitable).\n",
      "\n",
      "In business (hard to emulate in an online class), you tie (estimated) performance metric to a business outcome (deaths saved, engagement % increased, successful fraud attempts decrease, negative sentiment on timeline decrease, number of customer support tickets closed within one hour, % more people you can give credit), and often translate that outcome in hard cash, or lives saved when implementing new model vs. keeping old model.\n",
      "\n",
      "If you do not venture beyond evaluation metrics, then you don't really need to test model beyond the training-test set. You assume your test data is similar to unseen data, and that unseen data will not drastically change next month without retraining. If you actually deploy model and make (hopefully better) decisions based on it, you can do offline evaluation, simulation, small tests, but only deployment teaches you if assumptions hold, or if real world is too noisy and unclean from your laboratory. So you deploy the model, hope it is right, and carefully monitor and quickly debug any anomalies that are bound to pop up. Lots of data sets (I'd say majority of real-life datasets) have undetectable biases or leakage which don't show up or are missed during creation. Sometimes even the target labels can't be fully trusted or future-tested.\n",
      "\n",
      "There is specialized research on how to do better verification/learning when it is impossible to collect or create more labeled data (you can't give someone a disease to get a positive labeled sample, nor randomly experiment with medicine-interactions to create more data). So if you do get to experiment, how to best select your data? How to learn when outcome is much delayed? How to make uncertain predictions more certain, with least amount of samples? It remains a difficult problem.\n",
      "I agree to move on to calculus and probability next.\n",
      "That would depend a lot on the data.   How much do you have?  How long is each sample?   Do you expect long term dependencies?  Is there some kind of context for the data?\n",
      "\n",
      "I would start with some feature extraction and more traditional classifiers first. There are many libraries out there for that.  From there you could move into some deep networks, but I would start simple.\n",
      "Ditch the LSTM and first build a better-than-random very simple linear model like Logistic Regression, or tiny bit more complex with a random forest.\n",
      "\n",
      "Hand-craft the lagged and/or time-shifted features (featureC_3_days_ago: 42, maxFeatureB_over_last_week: 12) to make the model better at capturing patterns in time-series.\n",
      "\n",
      "If you start with LSTM, but haven't even created a baseline which predict target variable of last time-tick, you setting yourself up to fail or waste time and cloud compute credits.\n",
      "Why not use a GP? It would give you a good baseline since it seems like you have a pretty low-dim setup.\n",
      "It is close to recommendation system, but the interaction properties are closer to contextual bandit (RL) learning: With movie recommendation the user decides what to watch, with personalized education the tutor decides what question to ask. So it can make smarter use of exploration (\"I'm not certain if the student is good in this area, lets ask a few more questions there to get a better estimate of skill\"), and adaptation (\"This particular student always did very well on this tag and its foundations/basics, but recently starts struggling with more advanced questions on this tag, lets refresh the basics with a few beginner questions, and if student also struggles with these, there may be an external factor negatively influencing current problem-solving capabilities. Lets alert human teacher or refer student to ELIZA\").\n",
      "If the goal of the task is predicting future labels, we need to build a classifier (If the sole purpose is for clustering, e.g. using k-means for image compression, that is not necessary).The formulation of k-means clustering leads to the decomposition of the data space into Voronoi cells with linear boundaries, so a linear classifier is the most natural. Since the pseudo labels of k-means are linear separable, I suppose regularization is not necessary  (unless there are additional goals to the task, e.g. feature selection).\n",
      "For instance, you cluster your customers. Then you learn an interpretable model to predict the cluster assignments in multi-class (eg. one-vs-all) manner. If the clustering was good, and the classifier is good, then you now have unsupervised customer stratification, with explanations for what makes the clusters different from other clusters (what makes the cluster typical).\n",
      "\n",
      "Regularization is a parameter to tune for which you have data and labels, and its use and settings should be driven by data.\n",
      "\n",
      "Another reason for semi-supervised learning in this way, is to use the clustering to make it easier for the linear classifier to make good predictions. Clustering is non-linear (but can still be made very interpretable) and cluster assignments are good features or train data selection.\n",
      "\n",
      "Neural network research also use pre-clustering, to create expert models for each cluster, and aggregating or selecting these experts based on test data cluster assignment yields good results. Neural nets trained with pseudo-labels also still create useful embeddings in the later layers, which can be used for transfer learning --- no need to pre-train on ImageNet, can pre-train on unlabeled image data with pseudo-labels.\n",
      "Game over screens/deaths and \"game scores\" or \"time alive in seconds\" were traditionally hand-tied to the reward function the agent is optimizing.\n",
      "\n",
      "For instance: \"If you are currently alive [not Game Over screen], then reward a few decaying points to the actions you took before. If currently dead [Game Over screen] strongly penalize with decaying points the actions I took shortly before death.\"\n",
      "I'm not sure what you mean. Those are typically hyper params, are they not?\n",
      "Short Answer: you don’t know until you try it!\n",
      "Slightly Less Short Answer: You can look at what other people have done and copy their models with adjustments for your own data.\n",
      "According to [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/), symmetry breaking is one of the main reasons.  Basically, if you choose a constant initialization, all hidden nodes connected to the same inputs will learn the same function.  You'll get stuck essentially and it will limit the complexity of your network.  Other than that, most people just choose a nice Gaussian distribution that is large enough to break symmetry, but not too large to cause exploding gradients.\n",
      "Overfitting can at least be viewed graphically. I am not sure of statistical measures. \n",
      "One can compare the training loss vs validation loss, where both losses should decrease per epoch. There may be a point where the validation loss inflicts and increases, while training loss continues to decrease. That is an artifact of overfitting.\n",
      "Overfitting/underfitting are usually described in the context of the bias-variance tradeoff. Ignoring double descent for a moment, as your model complexity increases the bias decreases, and the variance increases. If your variance is huge then you aren't fitting the underlying data distribution, just learning to fit individual data points exactly. If you are perfectly fitting your training data and doing poorly on test data, then you are overfitting. Underfitting just means that you can't even fit the training set i.e. the bias is dominant.\n",
      "\n",
      "In terms of a statistical measure, you'll probably overfit if your model can shatter the dataset (or more).\n",
      "Nearly always. But needs:\n",
      "\n",
      "- to be natural or artificial data with at least some pattern / predictability / local structure. Random data is not meaningfully suitable for clustering.\n",
      "- to have imputed missing variables or adjust similarity calculation to ignore it.\n",
      "- to be able to be scaled and allow for meaningful comparison.\n",
      "- to have sufficient similar data to build clear clusters, especially when very sparse, such as text data with highly-weighted unique-word documents.\n",
      "- needs to fit into memory, and satisfy eventual latency constraints of production.\n",
      "It means it is really hard/not obvious how to efficiently do this. Would be very welcome (but not absolutely needed).\n",
      "Look into the DEEPFAKE Technology\n",
      "I would not agree with that.  Converting your data to an image seems like more of a hack.  You can create a specialized network from the components that are already out there.  That's most AI right?  They build a system out of pieces and the tricky parts are adding training strategies and other maths to help achieve the best cost function for the task.  Take speech for example.  The problem of generating realistic, arbitrary speech is almost solved.  Is that very different than music?  Speech is ultimately numbers, such as a spectrogram.  So, learn that instead.  Music will have analogous conversions.\n",
      "\n",
      "And, I'm pretty sure that a lot of the state-of-the art learners are using graphs to solve physics problems.  Not everything boils down to a convolution network.\n",
      "Current state of AI is Facebook and Google using deep learned computer-vision - and language models for better social media features, information retrieval, and ad-serving.\n",
      "\n",
      "If your data has recurring patterns, such as Go, natural text/images, protein folding, and music, then you can use DL NNs to solve it or generate it.\n",
      "\n",
      "Music generation is still active research field, but has much lower commercial value than, say, better face detection or language understanding.\n",
      "\n",
      "DL Neural nets do good on natural images, because natural images have recurring patterns (randomness is rare in nature, symmetry is common), and objects often have an hierarchical structure of smaller parts. DL methods such as convolutions, attention, many layers, bottlenecks, and pooling were designed to \"best\" capture these (\"best\" not in most robust and elegant for AI, but \"best\" for reading checks or classifying/adjusting ad-profiles from uploaded photes).\n",
      "Seems to apply classification or regression to a problem that would fare better with a reinforcement learning or game theory approach.\n",
      "\n",
      "This game sounds closer to Poker, where some cards are known, some are unknown, and some can only be put on a plausible range. You can only clearly learn that a starting hand of 7-2 is a poor hand, when you: A) saw it went to the final round without folding, and lost, B) do not often see such a hand at showdown (but lots of AA, AK, QQ, etc.).\n",
      "\n",
      "If you do want to build a classifier solution to this, before exploring more natural solutions, such as reinforcement learning, then a ranking-threshold method sounds reasonable. You could find the optimal threshold (or learn another model to optimally vary it, depending on context) through self-play or real-world evaluation.\n",
      "\n",
      "And incorporating confidence/variance is a great idea! (not sure if you meant it). Ranking by both predicted winning probability, and certainty of prediction, usually makes for more more solid decisions than ranking by win probability alone.\n",
      "> The softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1. The input values can be positive, negative, zero, or greater than one, but the softmax transforms them into values between 0 and 1, so that they can be interpreted as probabilities. If one of the inputs is small or negative, the softmax turns it into a small probability, and if an input is large, then it turns it into a large probability, but it will always remain between 0 and 1.\n",
      "\n",
      "> The softmax function is sometimes called the softargmax function, or multi-class logistic regression. This is because the softmax is a generalization of logistic regression that can be used for multi-class classification, and its formula is very similar to the sigmoid function which is used for logistic regression. The softmax function can be used in a classifier only when the classes are mutually exclusive.\n",
      "\n",
      "> Many multi-layer neural networks end in a penultimate layer which outputs real-valued scores that are not conveniently scaled and which may be difficult to work with. Here the softmax is very useful because it converts the scores to a normalized probability distribution, which can be displayed to a user or used as input to other systems. For this reason it is usual to append a softmax function as the final layer of the neural network.\n",
      "\n",
      "[https://deepai.org/machine-learning-glossary-and-terms/softmax-layer](https://deepai.org/machine-learning-glossary-and-terms/softmax-layer)\n",
      "If you think about a cloud of points, maybe in 2D, the covariance matrix will give you an ellipse that sort of “estimates” the data. In order for that matrix to be singular, one of the axes on the ellipse has to collapse to zero. This is all basically a really roundabout way of saying that if parts of your data are linearly dependent, you’ll get a singular covariance.\n",
      "As someone already said, singular means that some of your data is linearly dependant, so the determinant will be zero, giving no unique inverse. There is about 50 years of linear algebra to deal with inverting matrices approximately though. It happens all the time, just take a pseudo-inverse.\n",
      "Hey, I'm a ML newbie too!  \n",
      "Came accross [this](https://www.reddit.com/r/MachineLearning/comments/5z8110/d_a_super_harsh_guide_to_machine_learning/?utm_source=share&utm_medium=web2x&context=3) a while ago, hope it helps.\n",
      "If you just want to do applications do ML Coursera from Stanford, then either [https://course.fast.ai/](https://course.fast.ai/) or  Deeplearning.ai also from Coursera  depending on if you prefer a more top-down (application to theory) or bottom-up (theory to application) approach to learning. Then do an end-to-end project (data acquisition, cleaning, visualization, pre-processing, model selection and training, results evaluation and post-processing. Probably on the way you will need to learn many new useful, practical things you could not learn just through courses.  \n",
      "\n",
      "\n",
      "If you want to do research, the other comment may be a good starting point, but another possibility would be to just follow those courses I mentioned and then instead of a project go read papers (importart, landmark ones!), and see if you understand, then you're \"good\" to go. If you don't then go back to those books mentioned by the other comment.\n",
      "Use towardsdatascience. It must give you a good knowledge of current trends in the NLP world.\n",
      "What is your input? A picture of the website, or the source-code/URL? If you can access the website, you can calculate all objects and their locations exactly, with Chrome headless. Website are usually already divided in clear sections/squares, to aid with information consumption. Traditionally, you could move a horizontal \"line\" and a vertical \"line\" over a website, and if the \"gradient\" of change is constant you have a \"gutter\"/\"whitespace\" separation. If gradient drastically changes, then you found a section edge. But that's kinda hacky and only works with certain websites. Strong baseline though, for separating websites in sections (I think Google used this in early render-crawlers to see if advertisements were obnoxiously placed, or to find and remove boilerplate code/text).\n",
      "Likely a leakage issue in evaluation pipeline. Find the error, before putting more layers and pray. Could create a new small test set from the train set, and see if the same thing occurs or not. Should help with finding where it goes wrong. If you can't find the error, move a complete step back, and create 1-3 extremely simple baselines.\n",
      "TBH model *architectures* rarely make a difference if the baseline model is properly tuned. See nnU-net for an explanation [https://github.com/MIC-DKFZ/nnUNet](https://github.com/MIC-DKFZ/nnUNet)\n",
      "Not sure, but I think it is area of the mask/polygon. Not too useful to provide area of boundingbox as it is easy to calculate with width and height. Very useful to provide for masks, since area calculation is way more involved.\n",
      "\n",
      "To verify, find a picture with mask and boundingbox, then verify if area is equal to bounding box area or much lower.\n",
      "I personally like to weight the loss function based on the class presence in the training set. The class label with larger presence in the dataset gets weighted less. \n",
      "\n",
      "Don't know what dice loss is. Focal loss seems a bit of a overkill for me. If you can gather more data, focus on that instead IMO.\n",
      "In 2018 I applied for the professional master program and received an answer in March 2019 FYI. I wouldn't worry if you haven't heard an answer yet.\n",
      "Yes you can. You just need to attach some decoder layers after the given layer and add an extra term in your loss function to make the decoded image similar to the input image. You should take a look at autoencoders.\n",
      "Try a neural network with one hidden layer RELU function\n",
      "Where did you source your data from? I would like to take a look at it.\n",
      "How big is each image and how big is your dataset? If you can fit it on a large SSD local to wherever your GPU is, I recommend it.\n",
      "\n",
      "One thing to keep in mind with the storage format is whether or not using a lossy format will work out for you. Generally it's good to go lossless when you can, but it might make sense to e.g. keep your whole lossless dataset someplace remote and keep a local, preprocessed, possibly downsized dataset next to your GPU. 224x224 lossless images might take up less room, for example.\n",
      "Depending on the libraries you are using you can preprocess your images and save them in binary files (something like numpy arrays .npy). This would reduce I/O access and improve the data loading process. I'm assuming the bottleneck is related to reading the images from disk. If you are using Tensorflow look for TFRecord: https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
      "Good luck\n",
      "What you seem to describe here is feature engineering, which is a step usually done before implementing classical machine learning algorithms. Basically feature engineering is creating new useful features (variables) from the original data that you have in order to increase the efficiency of your machine learning model. \n",
      "\n",
      "Machine learning itself is about approximating a function that maps your inputs to your outputs. So you can think of it as: given some data (e.g.: tabular data in a dataframe as you described or images of animals), learn to map this to my desired output (e.g.: classes of animals).\n",
      "\n",
      "In a broader way, machine learning encompasses deep learning, which automatically learns complex representations of your features (variables), thus removing the need for feature engineering.\n",
      "\n",
      "> but also some new data that comes from your human understanding of the data in dataframe1 and how you can create new data that appropriately defines the relationship between two variables\n",
      "\n",
      "This could be right or wrong depending on the interpretation. You aren't creating anything new. You are pulling together many sources of data (features, representations) in order to discover some pattern or structure present in your data \"dataframe 1\".\n",
      "\n",
      "Machine learning is ultimately about prediction, in some form or another. _So in the most basic cases_ of regression/classification you are just taking in some large batch of data, and the associated true labels -> then you train your algorithm to predict those labels from the data. This gives you a model that you can use to predict the labels of new data. \n",
      "\n",
      "> Sometimes you leave out data from the original dataframe that isn't important\n",
      "\n",
      "Sometimes, sometimes not. There are variables (features) that you might not think are important, but end up begin quite useful. Machine learning is useful when data has correlations/structure that is hard to see with human eyes. But yeah, if you know some features are totally independent of the target label, then disregard them. \n",
      "\n",
      "The one time you \"create new data\" is with generative models. This is still a prediction, but the model takes in a smaller piece of data and generates something larger/new. There's lots of depth and details, but that's true somewhat.\n",
      "When upgrading Cuda, was cuDNN reinstalled as well? If cuDNN is missing, both forward and backward passes of convolutions will be slower. But I don't think it should be 10x slower---that's an extremely large change.\n",
      "Don’t use feature importance directly from the algorithm. Use SHAP scores instead.\n",
      "Could be the economic downturn affecting things in general. No new libraries with any market share. There's JAX, but its not used much compared to the big two.\n",
      "Random forests are useful when you have issues of scale (I tend to recommend those overly frequently so I will admit some biase). Another option that springs to mind would be to use a logarithmic scaler on your X variables and use a linear model.\n",
      "[Fixed formatting.](https://np.reddit.com/r/backtickbot/comments/l3daao/httpsnpredditcomrmachinelearningcommentskh2b81d/)\n",
      "\n",
      "Hello, GamerWael: code blocks using triple backticks (\\`\\`\\`) don't work on all versions of Reddit!\n",
      "\n",
      "Some users see [this](https://stalas.alm.lt/backformat/gkdob8t.png) / [this](https://stalas.alm.lt/backformat/gkdob8t.html) instead.\n",
      "\n",
      "To fix this, **indent every line with 4 spaces** instead.\n",
      "\n",
      "[FAQ](https://www.reddit.com/r/backtickbot/wiki/index)\n",
      "\n",
      "^(You can opt out by replying with backtickopt6 to this comment.)\n",
      "Dependso n data type. Are they regularly sampled or irregular? Is it from a sensor or real world measurements (e.g. times between customer interactions)\n",
      "Look into search methods, more than classification. \n",
      "\n",
      "I'd start with a strong baseline and see if that's enough, e.g. the basic search from any engine (e.g. apache solr / TF-IDF, word ngram similarity etc). If that's no t enough, try embedding models (word2vec) and nearest neighbours maybe. \n",
      "\n",
      "If you have a relatively small amount of accounts to classify to (e.g. less than a thousand account/classes is probably ok, assuming you have at least a few dozen unique examples per account that are correctly labelled), then you can do it as a multiclass supervised ml problem as well.\n",
      "Fourier transform maybe?\n",
      "I would look through the opencv examples.  There are likely some simple extractions you can play with here.  This seems like one of those projects where you could even create an average image and compare to that - i.e. basically the L2 distance in pixel space.\n",
      "On top of my head i can recall these: Multicollinearity, Interactions, Boosting, Bagging, Ensemble, Cross validation.\n",
      "\n",
      "You should also be well aware of various ml algorithms\n",
      "Since Deep Learning is a part of Machine Learning, you should consider learning these concepts to.\n",
      "The concept of the 'right representation'. Often ML/DL practitioners outright feed data into their models and expect great outcomes. While this generally works, it may be possible to obtain better solutions, if the input is represented in a different way or preprocessed somehow. The \"circle in cartesian = line in polar\" from the first chapter of the deep learning MIT press book is a nice example of this.\n",
      "Have you looked into transfer learning? You basically pre-train your network on a larger dataset then fine tune the network on your own smaller dataset.\n",
      "3 images is definitely too few, and will most certainly benefit from external data. Not sure if there is any specific research on this, but it is quite common and normal to augment small datasets with \"similar\" or \"relevant\" external data. For example, SRGAN, evaluated on DIV2K were trained on DIV2K + Flickr2K dataset. In kaggle competitions, it's often specified whether \"external data is allowed or not\". If it is, it's often a no-brainer to get additional data to boost generalization. I suggest you add external dog images from google or some other dataset to your own without any hesitation.\n",
      "You can try grammarly, or use google translate to see if it makes sense in your native language.\n",
      "If you’re a beginner to ML but not to maths try [“Mathematics for Machine Learning”](https://mml-book.github.io/) (the first half is basically the math principles in abstract and in the second half they’re applied more. I only read the first half then my semester started). I’d then suggest [this](https://doc.lagout.org/science/Artificial%20Intelligence/Machine%20learning/Machine%20Learning_%20A%20Probabilistic%20Perspective%20%5BMurphy%202012-08-24%5D.pdf) which is overall more practical and applied, it was one of my ML course’s core books.\n",
      "It is possible, just not feasible. You'd have to hallucinate so much up to the point the network isn't really robust.\n",
      "\n",
      "What is completely possible is to build a network that can transform the footage to appear correct - rotating it, adjusting the white balance and coloring, smoothing out rough transitions. But it's just not feasible to build something that can bring it up to the quality of the original without memorizing the whole original.\n",
      "Does it have an overview of different architectures? And the Intuition behind them? Also do they explain Convolutional Neural Networks?\n",
      "Okay I'll check. A friend of mine recommended Gilbert Strang's Linear Algebra course.\n",
      "\n",
      "Thanks for your response.\n",
      "Thank you for answering. On the computational issues I said not 'too much of an issue' so my implied meaning was it has to be finite and it must be feasible if not now then in the near future :D\n",
      "\n",
      "Big breakthrough-wise (I think you meant algorithmically and not something like quantum computing) I think 0-0.5 breakthroughs. I kinda think we have the foundations now (disclaimer: I'm still learning AI). We do need some significant innovations of those foundations and its integration. But with the concept of transformer and metalearning as some of its foundations I can smell it already. Whereas 10 years ago I probably would think that AGI is just a pipe dream that would come half a century later.\n",
      "**[Categorical distribution](https://en.wikipedia.org/wiki/Categorical distribution)**\n",
      "\n",
      "In probability theory and statistics, a categorical distribution (also called a generalized Bernoulli distribution, multinoulli distribution) is a discrete probability distribution that describes the possible results of a random variable that can take on one of K possible categories, with the probability of each category separately specified. There is no innate underlying ordering of these outcomes, but numerical labels are often attached for convenience in describing the distribution, (e.g. 1 to K). The K-dimensional categorical distribution is the most general distribution over a K-way event; any other discrete distribution over a size-K sample space is a special case.\n",
      "\n",
      "[About Me](https://np.reddit.com/user/wikipedia_text_bot/comments/jrn2mj/about_me/) - [Opt out](https://np.reddit.com/user/wikipedia_text_bot/comments/jrti43/opt_out_here/) - OP can reply !delete to delete - [Article of the day](https://np.reddit.com/comments/k9hx22)\n",
      "\n",
      "**This bot will soon be transitioning to an opt-in system. Click [here](https://np.reddit.com/user/wikipedia_text_bot/comments/ka4icp/opt_in_for_the_new_system/) to learn more and opt in.**\n",
      "Will do, thanks for the feedback!\n",
      "Brilliant. Thank you.\n",
      "Thank you very much. \n",
      "\n",
      "The data is already one hot encoded – I’m passing it through several layers of a pipeline, and each stage is represented by a (N, 9,9,X) tensor for some X. Immediately after the one-hot encoding, X = 10. Then I mix in spatial information by computing 6 averages for each of the X channels, to give 6X channels. Then I apply Dense to each of the 81 cells to give 64 channels, then spatial mixing again to give 6*64 channels, then Dense, etc.\n",
      "Thanks! I'll look into that book. Probably gonna read it tonight. Also, I did start off with sklearn but to me at least it seemed harder to grasp and understand than Keras, although now knowing more in this field I could probably go back and try again. Also, for now, I'm just building some simple NN to do things like differentiating between a cat and a dog. Thank you though for taking the time to read it and give me an answer, really appreciate it! Hope you have a great day.\n",
      "Agree, data engineer. But solid software engineer with industry experience and machine learning expertise? Want!\n",
      "Except for batch size, right? That can change from training to testing since the samples in the batch don’t depend on each other.\n",
      "This is interesting because I am more interested in the computation side. My interest in robotics specifically stems from what robots can accomplish, not the robots themselves. I am more interested in deep/reinforcement learning as an independent topic to study than robotics. Could you elaborate on why there is no computer science left and how one can throw money at the control aspect though please, this could help me decide on which path I should take in university.\n",
      "That makes sense. I will do that. Thank you for the reply\n",
      "Actually I wanted to hear from others who had done such thing, if you want to know my own opinion, after asking this question I found that torch.optim.lr_scheduler.ReduceLROnPlateau which does this thing. How well it perform with Adam or SGD (or other optimizer) will depend on the data, architecture etc. But there exists such function already.\n",
      "Yeah, I know that Tesla is only an accelerator. I also found just now Tesla M40 that won’t kill my budget... I want to start learning Machine Learning, in addition to CUDA C and OpenCL.\n",
      " \n",
      "\n",
      "interesting, can you please share me the link to the guide (if any)? thanks\n",
      "Thanks for the feedback. I've been thinking about this and I agree.\n",
      "That's sad 😔 I had a similar experience in my last job, but trying to replace Ruby with Go.\n",
      "\n",
      "However, I think that it depends on the specific circumstances of the business and its history. I mean, maybe it's difficult for medium/large size businesses to switch to new technologies, even after recognizing the value of the new toys.\n",
      "\n",
      "Whatever. I still want to learn Rust... But also Python\n",
      "Thanks for the reply, Eric. I have double checked and re-run everything and can confirm I am connected to a GPU, but still receiving the errors\n",
      "great, thanks.\n",
      "Hasn't PPO been phased out already by the likes of SAC?\n",
      "Thanks a lot - I will try!\n",
      "I had some data on which I was comparing the results of K nearest neighbours with SVM (data input to both KNN and SVM was scaled). The k nearest neighbours method I use takes into account distance information (not the direct maximum voting k nearest) and I wanted to get its classification performance close to that of SVM if possible. Applying kernel transformation seemed like an option. I found this paper (direct paper link: http://goo.gl/xOj0b) which said it could possibly better SVM  and I think applying some transformation on data prior to computing euclidean distance would give the kernel effect. I am not clear on how to transform the data though for this.\n",
      ">Bertseka's book\n",
      "\n",
      "Thanx.\n",
      "Thank you sir.\n",
      "Thank you so much for the detailed response and suggestion! Basically, I'm going to do preprocessing to cleanup some banned classified samples before keeping them as a training set.\n",
      "\n",
      "And yes, by noise I'm mainly talking about messages that should be banned but the mods couldn't catch because of sheer volume. Ideally it'd be better to monitor a user as a whole and predict when they tip the scale of being not banned to most likely banned, but I suppose it'd be simpler to just do it on a message by message basis. Although that's not always the case, it probably is more often than not.\n",
      "\n",
      "And when a user is banned all their comments get removed from the Database. So I'm guessing another way of cleaning up data would be only taking their last emssage or alst couple messages and using that as the banned sample set? Although that'd probably require trial and error to really know\n",
      "I see, thanks a lot!\n",
      "I have tried replacing the f string but that is leading to other strange errors with different parts of the code that were working fine before when using python2. I am using python2 because if I use python3, most of the code is unusable because it is using deprecated tensorflow elements. I didn't write the code, as it is written by the model authors, so I don't want to attempt to rewrite it all. I was hoping there might be an import-related fix for this\n",
      "[Fixed formatting.](https://np.reddit.com/r/backtickbot/comments/ksm26h/httpsnpredditcomrmachinelearningcommentskh2b81d/)\n",
      "\n",
      "Hello, comradeswitch: code blocks using triple backticks (\\`\\`\\`) don't work on all versions of Reddit!\n",
      "\n",
      "Some users see [this](https://stalas.alm.lt/backformat/gigq21s.png) / [this](https://stalas.alm.lt/backformat/gigq21s.html) instead.\n",
      "\n",
      "To fix this, **indent every line with 4 spaces** instead.\n",
      "\n",
      "[FAQ](https://www.reddit.com/r/backtickbot/wiki/index)\n",
      "\n",
      "^(You can opt out by replying with backtickopt6 to this comment.)\n",
      "Wow, first of all, thank you for such an elaborate response.\n",
      "\n",
      "My learning rate was defaulted to 0.001 using Adam optimizer. Would that still be considered \"too high\"?\n",
      "\n",
      "My computer has shitty RAM (4 GB) hence I am left with no choice but to scale down my images to 100px otherwise the program will either take extremely long to run (took me 3 hrs to run 20 epochs with 26000 images) or crash. I ran matplotlib on some of my images after scaling for me to visualise and see if they actually still look like cats and dogs to ensure that at least most of the details are not lost. Ive tried halving the training sets while using more pixels but it resulted in significantly lower accuracy. \n",
      "\n",
      "I will definitely try out CV! Thanks for bringing that up!\n",
      "Thank you for answering !\n",
      "\n",
      "  \n",
      "The actual problem is more complicated. For instance, the sequence length for one is in the 100s. Afaik, simple NN might struggle with recognizing patterns in a position independent way on such a long sequence length. Which is why I looked towards self-attention mechanisms which lead me to transformers.\n",
      "Awesome! Thank you!\n",
      "Sorry for the late reply and thanks very much for your feedback. Interesting to see that the approach makes sense to you if tackled from the descriptive angle.\n",
      "\n",
      "To add more complexity to my question: I am now using a **very small tree** (max\\_depth=3) to have the best interpretability as possible. To find what features to use, I evaluate the tree right after training (on the same sample used to build the tree). Finally, I compare the predictions of the best tree with the actual labels from the train set. I am thus building this model as a kind of outlier detector. \n",
      "To sum up, I am using the **same train set** for training and predicting. I believe it still makes sense to do so here since the tree is very small so there is no risk of overfitting (actually there might be a risk of underfitting!). Do you agree?\n",
      "\n",
      "I see you've posted GitHub links to Jupyter Notebooks! GitHub doesn't \n",
      "render large Jupyter Notebooks, so just in case here are \n",
      "[nbviewer](https://nbviewer.jupyter.org/) links to the notebooks:\n",
      "\n",
      "https://nbviewer.jupyter.org/url/github.com/scikit-learn-contrib/skope-rules/blob/master/notebooks/demo_clustering.ipynb\n",
      "\n",
      "https://nbviewer.jupyter.org/url/github.com/interpretml/interpret/blob/develop/benchmarks/EBM%20Classification%20Comparison.ipynb\n",
      "\n",
      "Want to run the code yourself? Here are [binder](https://mybinder.org/) \n",
      "links to start your own Jupyter server!\n",
      "\n",
      "https://mybinder.org/v2/gh/scikit-learn-contrib/skope-rules/master?filepath=notebooks%2Fdemo_clustering.ipynb\n",
      "\n",
      "https://mybinder.org/v2/gh/interpretml/interpret/develop?filepath=benchmarks%2FEBM%20Classification%20Comparison.ipynb\n",
      "\n",
      "\n",
      "\n",
      "------\n",
      "\n",
      "^(I am a bot.) \n",
      "[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n",
      "[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n",
      "[^(Author)](https://johnpaton.net/)\n",
      "Thanks for those nice references!\n",
      "In conclusion, you should always see performance metrics relative to the input data and the decisions based on its output.\n",
      "\n",
      "A 90% accuracy can be had by a model always predicting YES, and a dataset with 90% YES and 10% NO. A person-of-interest/terrorist classifier may be highly accurate, but if a false positive costs a human rights violation/innocent droned to death, and a true positive only is valuable for confirming already existing hunches, then the cost-benefit could quickly outweigh the high accuracy. While a skin-cancer classifier with the very same accuracy, and false-positive rate may save many more lives when implemented (the cost of a false-positive is predominantly a doctor wasting a minute to discard it, in rare cases the patient receives unneeded medication or treatment which does no life-threatening damage, and some patients may be psychologically hurt by a fake diagnosis.)\n",
      "So the data I am working with is supply chain data. The target variable is daily demand for a product. I have access to other features to pair up with the target. I would assume there is some long term dependence relating to seasonality of the product but this may not be the case for all products. I have an lstm which isn’t giving great results\n",
      "It seems somewhat redundant to train a supervised learner on data you separated with k-means does it not?   Maybe if you ran some kind of cross fold pipeline.\n",
      "Would you use and l1 norm regularizer or l2 norm regularizer?\n",
      "Ah so they were manually coded?\n",
      "What about glorot initialization\n",
      "Also, convolutions are so 201x. Attention is all you need; long live transformers!\n",
      "Thank you for the insight, I truly appreciate it. For context, in this game each player will only have 1 card. And yes, it is similar to poker in which you know the ranges that some players cards may be in based on the context of the round.\n",
      "\n",
      "If you don't mind elaborating, how would you calculate a certainty of prediction. I'm familiar with calculating confidence intervals for a normal or Gaussian distribution, but I'm not sure that's what you mean. It seems, given a training set with feature vector X and an output vector Y of boolean 1 for win or 0 for loss, that this would follow a sort of inverse Gaussian distribution, if that makes sense. Where, given a very low card or a very high card, your certainty that you will lose or win, respectively, will be very high; likewise, given a middle value card, you are much less confident in either outcome.\n",
      "Thank you for you answer. It's really interesting. I didn't know about it since we always used in classe the sigmoid function and I forgot to specify it in my question.\n",
      "\n",
      "Do you know other functions that are useful/are preferred in some cases for the hidden layer please ?\n",
      "Thanks.  I luckily just seem to understand most every comp sci thing once I've had it explained well once or twice.  I just need resources that can explain it well.  Im at a loss to understand without really tearing apart a projects code but that always leaves gaps in knowledge so I guess I'm out to learn from theory to application.\n",
      "Input is a screenshot of the webpage. Cannot use html/source-code. Like I want to know if anyone has done any research and/or written a model for this use case before and if anyone has then which is the best one?\n",
      "My thoughts exactly.\n",
      "Thank you so much for this comment. Now I know that I should be looking into autoencoders.\n",
      "Thanks for the response. Can you point me in the direction of a resource that would get me started on this?\n",
      "I got the state data from here: [https://www.kaggle.com/nightranger77/covid19-state-data](https://www.kaggle.com/nightranger77/covid19-state-data), and the obesity data is here: [https://drive.google.com/file/d/1PAcAz4zoYMm95VKo6RB5JVSMpRU5xNTa/view?usp=sharing](https://drive.google.com/file/d/1PAcAz4zoYMm95VKo6RB5JVSMpRU5xNTa/view?usp=sharing) (I couldn't find the original source)\n",
      "\n",
      "Btw, turns out I had my x and y values reversed, and after taking out the outliers, I have much better results\n",
      "\n",
      "\\`Mean Absolute Error: 0.00458312091656227\n",
      "\n",
      " Mean Squared Error: 3.085310143218458e-05 \n",
      "\n",
      "Root Mean Squared Error: 0.005554556816901289\\`\n",
      "\n",
      "still don't know what they mean though\n",
      "The dataset is of 15M images. Images are in PNG format with the width = 1280px, and height = 700-2000px. The images size is 200KB to an MB.\n",
      "Yes, this seems like a good solution. Thanks for the answer!\n",
      "Depends on the algorithm. Permutation, boruta, are fine for example. And with some GBM models, the model importance is based on loss (e.g. xgboost and GBM models) rather than gain/gini, and that correlates reasonably well with feature importance. \n",
      "\n",
      "Here's a great example of how entropy/gini can be meaningless and why! : \n",
      "\n",
      "[https://scikit-learn.org/stable/auto\\_examples/inspection/plot\\_permutation\\_importance.html](https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html)\n",
      "Do you mean because of the Covid?\n",
      "Stating the obvious here but man this is a HUGE field. I wrote software for 12 years at Dell, Callaway and another small software firm and the terms involved and data science in AI/ML is just unreal, awesome stuff. I did just find this new company called Levity which seeks to bring ML to mortals. https://www.levity.ai/\n",
      "Yes the batch size can vary. Batches are used to optimize training performance and if your GPU memory allows you can use a larger batch size. But research suggests that smaller batch sizes are better (they provide a regularization effect). Batch size = 32 usually works the best in a wide number of cases.\n",
      "I think career advice depends on where you are physically located and how rich your parents are. E.g., if your parents can afford a Silicon Valley house on walking distance from Google, you might do something different than if you live in Bangladesh. \n",
      "\n",
      "A lot of these machine learning papers fail to properly compare with *all* of the state-of-the-art methods from the past (papers have been written about this that show that essentially no progress has been made). Meanwhile, every day new methods come out that claim to be beating the state-of-the-art. Both can't be true at the same time. \n",
      "\n",
      "In reality what's happening is that those parties with the most *capital* to employ are \"best\". So, what you need in order to succeed is not more hours studying linear algebra, but more skills to attract capital or to simply work for someone that has already done that work for you.\n",
      "\n",
      "The only point to publish is to signal to investors that \"our AI is the best\", even if the money making algorithms (ad platforms) likely won't see much more improvement, because the data sets are inherently noisy and incomplete. \n",
      "\n",
      "In the 1960s AI researchers had already invented methods that are more general than the AI methods we have today, but computers were too slow to run these and to this day that's still the case. \n",
      "\n",
      "Computer science is a cold hard science, which ultimately boils down that a lot of functions can *not* in fact be computed. Meanwhile the machine learning crowd is claiming that \"given enough training\", it will perform miracles. Clearly, both can't be right. \n",
      "\n",
      "Does that mean that AI cannot do things that people do in a lot of jobs that involve following simple commands? No. AI can do all of those things, but there are a lot of functions that no neural network can perform using 2020s technology.\n",
      "\n",
      "For the control aspect, you can just run a thousand of those robots in a virtual environment with ten or so in a real environment to collect data. There is no computer science involved there.\n",
      "No issues. I myself have been interested in robotics for years now and currently working as an ML Engineer in Computer Vision. So that's another path you can look into. Hope you find your niche. Good luck!\n",
      "I was wondering why you cared about FP64 performance, thanks for clarifying! Good luck!\n",
      "https://ubuntu.com/blog/ubuntu-on-wsl-2-is-generally-available\n",
      "\n",
      "Also works for eGPU setups (run it on Win10 (WSL2 Ubuntu) -> Bootcamp -> MacBook Pro setup, yes don’t judge me :)\n",
      "I’ll take a look later this morning. I have nothing better to do.\n",
      "Ah, cool. I see. I am not sure how you would transform the data either. In SVM the data are transformed to create a margin that separates the data in higher-dimensional space. In KNN we don't have margins. So I suppose the strategy would be to transform the data in higher-dimensional space to minimize some loss function, like misclassification rate. It might be possible to code up an implementation from scratch. Perhaps there is a routine available in some statistical software.\n",
      "\n",
      "This doesn't answer your question exactly, but you might be interested in GAMs and MARS. In generalized additive models you transform each of the variables, usually using cubic smoothing splines. MARS is very similar except it uses a different spline-fitting procedure, the MARS algorithm, which fits piecewise linear models to the data. Both can be adapted to discriminant analysis (so unlike linear discriminant analysis, or quadratic discriminant analysis, which model the data using linear and quadratic decision surfaces respectively, in flexible discriminant analysis one can model the decision boundary using any arbitrary nonparametric regression strategy). The reason I suggest GAMs and MARS, and FDA, all three rely on transforming the data, so that if SVMs give good results, those other models are worth a try.\n",
      "> Ideally it'd be better to monitor a user as a whole and predict when they tip the scale of being not banned to most likely banned, but I suppose it'd be simpler to just do it on a message by message basis\n",
      "\n",
      "It doesn't have to be either or! You can simultaneously model messages and users. Imagine having two distinct sets of labels- for each user, you have whether they were actually banned, and for each message you have whether it was worthy of a ban or not. You know the user labels already. The message labels are unknown, but you can connect those. P(message is ban-worthy | user was banned) is going to be greater than P(message is ban-worthy | user was not banned) but the latter will not be zero. Likewise, P(message was not ban-worthy | user was banned) is likely to be quite high- banned users are much more likely to send innocuous messages anyway, that's the nature of communication. Then, as you noted, a ban-worthy message is much more likely to be close in time to the user being banned. The simplest approach would be initially to look at the last t amount of time before a ban for a user and see what the differences are between those messages and ones from users who weren't banned, but you could also incorporate some kind of decaying weighting based on the time until a ban.\n",
      "\n",
      "It sounds like this is for an ongoing, active service, yeah? If it's still possible to collect moderation data, would it be feasible to ask moderators to \"explain\" a ban, giving a list of messages that resulted in the ban and ideally words or phrases that specifically highlight the behavior? If not, could you look at some messages prior to bans manually and label them? Every little bit of annotation can help quite a bit, semisupervised methods can do some pretty amazing stuff with a little help. You can also make use of active learning methods, where you train a model and then use some form of criteria to select messages (like ones that are most uncertain) to present to a human for manual annotation, then continue training and presenting the most \"helpful\" examples for annotation. That can drastically improve the results with little effort because your manual annotation is focused on difficult examples and not wasted on obvious ones.\n",
      "\n",
      "There is a good amount of research and practical tools for identifying abusive/hateful speech, which could be useful for you. The standards for moderation are probably going to deviate from whichever tool you use but that could be a very helpful way to hone in on the specific messages that are likely to be relevant to moderation. Something as simple as a list of hateful slurs could help you bootstrap the annotation to the point where you can extend it with the unlabeled data.\n",
      "That's what the cell magic does. Import TensorFlow after running the magic in a Python 3 instance.\n",
      "> My learning rate was defaulted to 0.001 using Adam optimizer. Would that still be considered \"too high\"?\n",
      "\n",
      "It's something that really depends on the problem. Adam and similar modern optimization routines are pretty smart about things generally but it's something you usually just have to fiddle with and see what happens. \n",
      "\n",
      "Regarding memory usage, there are some tricks you can use to make the most of it. What data type are you using for the pixel information? There's a good chance that you could convert to a 16 bit number without losing much information if you aren't already using them. At f64, you'd be using 80 kb per image (so 2.08 GB for 26k images), at f32 it'd be half that, and f16 would give you a quarter which would mean all 26k images would occupy only 520 MB or so. It will depend on your hardware, but the actual arithmetic operations may be quicker than with higher precision numbers. Using f32 will definitely be faster than f64 all else equal, but your cpu may or may not be able to do the same optimization and vectorization with f16. You may see a speedup regardless since your data will take up less space, but you should try that out. Keep in mind that converting back and forth between different precisions is slow- if you want to try a different precision, you need to make sure that your model parameters are also in that precision and that you aren't doing arithmetic with, for example, target labels that are in a different precision.\n",
      "\n",
      "The other big thing you can do for memory usage is to only keep some of the data in memory at the same time. You don't really need all 26k images available all the time- you could split that into smaller chunks, save them to disk, and then read one, train an epoch on it, replace it with the next, train an epoch on that, and cycle through all the chunks. Adam is a stochastic optimization method anyway- it's choosing a small number of samples randomly to approximate the full gradient at each step, so it's not using the full dataset every update. Just make sure you have even proportions of each class in the chunks.\n",
      "\n",
      "However, I think with your comparatively limited computing resources you may be best served by using a non-neural method for classification directly or to do dimension reduction/feature selection as a step prior to the neural model. I'll be perfectly honest here- I'm not exactly sure where I'd start in that direction, I don't have any experience with image processing in that regard, but there are a wealth of techniques for general dimension reduction, edge detection, image segmentation (splitting images in chunks that represent higher order concepts) and things like variational autoencoders that can learn a way to compress data that preserves the important information about it. Someone else here may have more informed suggestions.\n",
      "\n",
      "Generally, the best way to approach a problem is to start with the simplest possible idea you can think of and try it out. If nothing else, you can use that as a baseline- if a more sophisticated technique doesn't improve on the baseline, you know that it's not worth the complexity or that you're missing something that is preventing it from learning. If you can do your classification task well with something like an SVM or nearest neighbor classifier, then you have a much simpler method that is faster to train and evaluate (and therefore tune) and you may be able to make use of more data for that. Starting simple also means you can get a feel for the important aspects of the problem and make more informed choices about what you try next. So my recommendations for simple approaches- try an SVM classifier on the raw data first, then run SVD on the images to get a lower dimensional representation and try an SVM on that, and then use the SVD representation as an input to a neural model. Do your best at each step to get the best results, and move on when you are satisfied that you can't do better. SVD is very efficient, you should have no trouble with that, and you can compute a larger number of components than you think you'll need- using fewer components is as simple as taking the first n columns of a matrix instead of the whole thing. You don't need to recompute the SVD. SVM is a very efficient classifier that you can use a variety of kernel functions with. It's fundamentally a linear technique, but there's a powerful tool called the kernel trick that allows learning nonlinear relationships with the same complexity. It's a convex optimization problem, so there are very fast solvers available and you're guaranteed to find the optimum solution. There are maybe 3-4 hyperparameters tops that you need to tune, and most of them are pretty forgiving in their settings unlike the extremely fickle neural models! \n",
      "\n",
      "Last thing- look at your neural model results- calculate the confusion matrix. Are you consistently missing a particular class? (Like, are your errors mostly mistaking cats for dogs?) You might need to weight classes differently then. How does the loss (cross entropy or whatever you're using) look on the validation set? It's possible that looking at accuracy alone is obscuring improvement on the probabilities. Making a prediction of .51 for the correct class vs .49 changes the accuracy, but isn't very different, while .51 vs .99 is certainly a worse result but it doesn't change the accuracy. Visually inspect the images that you're getting wrong, and the ones you're unsure about. Is there something going wrong with your preprocessing, or errors in the labels? Do be careful not to try to \"peek\" at the results for the purposes of changing hyperparameters, this is strictly for making sure you're giving the model correct labels and good data.\n",
      "If position independent consider pre-processing to a different representation. No need to make the network figure out e.g. counting letters if that is really what matters and it’s easy for you to do. CNN is also position independent.\n",
      "So, I'm not sure. Perhaps someone else with more experience can add their thoughts. \n",
      "\n",
      "I agree underfitting the model with a small tree is the best way to perform a descriptive of analysis, since the deeper branches will be overfitting to the specific dataset, which in this case will lead to generalizability issues. The tree branches will give you the information gain associated with each split, which you can use to understand your dataset. In this case the decision to use your whole dataset makes sense since the information gain is purely describing your dataset. \n",
      "\n",
      "Using it to predict outliers is where I'm uncertain. I feel like it might be too underfit to provide reliable results, and furthermore, if the goal is inference, it seems like you should be validating your models on a test sets. What's the value here of doing everything on the training set?\n",
      "LSTMs are tricky to train.  They can give some good results, but have trouble if the sequence becomes too long.   You need to be carful to manage the states correctly between batches and such if necessary.  \n",
      "\n",
      "Can also try 1D convnets.  \n",
      "\n",
      "Are you using something like MSE as a loss function?\n",
      "I suppose you're worrying about \"using the data two times\" thing, which is valid, like if we do hypothesis testing on the learned cluster then the p-value might be inflated.\n",
      "\n",
      "The context of this is simpler though. k-means already comes with a Voronoi decomposition, which is equivalent to a linear classifier that can be used to make prediction (by 1-nearest neighbor).\n",
      "I'd probably use whichever evaluation tells me works best. Then ask someone with more knowledge on theory which regularizer would make sense for the data, model, consumption, etc. and hopefully get theoretical confirmation/justification for my usage/increased performance.\n",
      "> how would you calculate a certainty of prediction.\n",
      "\n",
      "There are various methods for this (variational inference), and some models are easier to get the confidence, or even provide as part of algorithm.\n",
      "\n",
      "A very simple method is to train 10 models on different sample and feature subsets. The prediction then is the average of the model predictions, and the confidence is the standard deviation of the model predictions (higher is less confident). The model may give same win probability for different contexts. If the context was seen very rarely, the confidence will be low, if the context is very common, the model will be sure. It may be good strategy to fold high-win probability cards, when the model is very uncertain (it has just seen the card in that context once or twice in training, and it won both times, so high win probability, but third time may easily lose).\n",
      "You might find [Francis Chollet's autoencoder blog post](https://blog.keras.io/building-autoencoders-in-keras.html) to be a great place to start digging into the subject. Depending on your goal, some type of VAE specifically might be of interest for reasons outlined in the article.\n",
      "Sure. Try Pytorch, follow a tutorial for neural network\n",
      "Right, could be part of it.\n",
      "Thank you!\n",
      "`dnnlib.SubmitConfig` clearly does not exist in that version of dnnlib, [and looks to have never existed in the `NVlabs/stylegan2-ada` repository](https://github.com/NVlabs/stylegan2-ada/search?q=SubmitConfig). However, [it does exist in the `NVlabs/stylegan2` repository](https://github.com/NVlabs/stylegan2/search?q=SubmitConfig). My hunch is that the code was haphazardly ported from StyleGAN2 to the newer StyleGAN2-ADA, and it is simply an oversight after porting. [There is an issue in the `jeffheaton/t81_558_deep_learning` repository](https://github.com/jeffheaton/t81_558_deep_learning/issues/87) (I assume you are you eluzzi5?), so I'll add this info to that issue.\n",
      "\n",
      "Edit: I added that info to the issue, so I would just ignore that code cell right now until it is fixed. Everything else seems to work.\n",
      "I ran that in colab, then in terminal ran python encode\\_images.py aligned\\_images/ generated\\_images/ latent\\_representations/. \n",
      "\n",
      "That caused me to get AttributeError: module 'tensorflow' has no attribute 'Dimension' (because Dimension is deprecated)\n",
      "\n",
      "Then in terminal, I tried pip3 install tensorflow==1.15, which led to the following error while running the code:\n",
      "\n",
      "ModuleNotFoundError: No module named 'tensorflow.keras.layers.experimental.preprocessing'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/content/stylegan-encoder/encode\\_images.py\", line 11, in <module>\n",
      "\n",
      "from encoder.perceptual\\_model import PerceptualModel\n",
      "\n",
      "  File \"/content/stylegan-encoder/encoder/perceptual\\_model.py\", line 3, in <module>\n",
      "\n",
      "from keras.models import Model\n",
      "\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/keras/\\_\\_init\\_\\_.py\", line 6, in <module>\n",
      "\n",
      "'Keras requires TensorFlow 2.2 or higher. '\n",
      "\n",
      "ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via \\`pip install tensorflow\\`\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Is it possible that encode\\_images.py was written in a way such that the python version and tensorflow version required are not compatible?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Thank you for all your help thus far, by the way. I am a ML beginner so I really appreciate it!\n",
      "Thanks for the comprehensive explanation, once again!\n",
      "\n",
      "You might be right about using 16 bit floats. Currently, Im using 32 bit floats. Hopefully halving thr bits will reducr memory use.\n",
      "\n",
      "Never really had experience using SVM models before, but Ill definitely explore more of it!\n",
      "\n",
      "Thank you for your insight! I will occasionally come back and look through your comments again to review my code haha\n",
      "Thanks for the feedback, much appreciated. Doing everything on the train set doesn't bring much value indeed, but I felt it makes sense to use the same rules I highlighted during the descriptive phase for the prediction.\n",
      "How long is too long for LSTM? What if it’s stacked LSTM? I’ve got 2 years of data at the daily level. \n",
      "I’ll check out conv v next. \n",
      "Also, is stationarity required for these models like it is for more statistical approaches like ARIMA?\n",
      "I guess I'm thinking more that even if the labels I assign aren't that meaningful, if I ask k-means to separate data into 2, then of course any classifier I train using those values will have insanely high accuracy.\n",
      "\n",
      "But, I get what you're saying.  Unseen data can be assigned so we can assess how meaningful the clusters are that way (or through a silhouette analysis maybe or both).\n",
      "\n",
      "I've seen this come up a lot recently in medical domains where they don't believe the assigned labels and aim to assign their own and show that they are more meaningful and predictive.  I see a lot of studies where they just cluster all of the data and THEN run a classifier on all of those labels, without any hold-out data on the original clustering.  Seems to me that doesn't prove a whole lot and suffers from massive data leakage.  I'm getting off topic now, but it seems more interesting if you could somehow correlate the pseudo-labels with some other measures to show that they are in fact meaningful, but also construct some pipeline such as:\n",
      "\n",
      "data -> cleaning -> k-means on training -> classify on validation\n",
      "\n",
      "And run that 10 times, etc...\n",
      "Any resources mentioning this equivalence?\n",
      "Thanks so much for helping out. Cheers\n",
      "Thanks. I will do so!\n",
      "I can PM you some code I know works if you would like, but I would need to clean it up for distribution. (It was very much written on the fly, and there is some personal information in there that I would need to remove.)\n",
      "In my experience, over 1000 you start to run into trouble but that’s problem dependent I’m sure.  Clipping gradients and other training tricks can help.  They have there limits, which is why other networks like wavenet have been developed. \n",
      "\n",
      "Anyway, I have found no trouble training it to learn non-stationary trends.\n",
      "That would be absolutely amazing. Thank you so much! I truly appreciate it\n"
     ]
    }
   ],
   "source": [
    "submission.comments.replace_more(limit=0) #Comment forest\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
